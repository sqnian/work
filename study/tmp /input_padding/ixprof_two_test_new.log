nohup: ignoring input
test op 

output_pt:tensor([0.1268, 0.0000, 0.2979, 0.2075, 0.0000, 0.0000, 0.1240, 0.0099, 0.0000,
        0.0629, 0.0126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1216, 0.0000,
        0.0000, 0.4124, 0.3174, 0.2001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.1678, 0.0000, 0.1964, 0.1820, 0.0000, 0.0000, 0.2285, 0.0276, 0.0966,
        0.0000, 0.0000, 0.6958, 0.1216, 0.0000, 0.0997, 0.0000, 0.2795, 0.1609,
        0.0000, 0.0891, 0.2732, 0.0000, 0.0000, 0.0000, 0.0000, 0.1895, 0.3425,
        0.2578, 0.0000, 0.0000, 0.0000, 0.4500, 0.0000, 0.1271, 0.0106, 0.0602,
        0.0000, 0.2917, 0.0000, 0.1383, 0.0000, 0.0000, 0.1998, 0.0000, 0.0000,
        0.6367, 0.0000, 0.0000, 0.4355, 0.0000, 0.1749, 0.0000, 0.0000, 0.1642,
        0.4077, 0.0078, 0.0000, 0.4426, 0.0000, 0.1885, 0.1078, 0.1750, 0.0000,
        0.0000, 0.0273, 0.0922, 0.0000, 0.0000, 0.1260, 0.6016, 0.0000, 0.0000,
        0.0378], device='cuda:0', dtype=torch.float16,
       grad_fn=<SliceBackward0>)

output_pt shape:torch.Size([24, 176, 19, 256])
output_cu:tensor([0.1270, 0.0000, 0.2979, 0.2075, 0.0000, 0.0000, 0.1241, 0.0099, 0.0000,
        0.0628, 0.0126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1216, 0.0000,
        0.0000, 0.4124, 0.3174, 0.2002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.1681, 0.0000, 0.1964, 0.1820, 0.0000, 0.0000, 0.2286, 0.0277, 0.0966,
        0.0000, 0.0000, 0.6958, 0.1216, 0.0000, 0.0996, 0.0000, 0.2793, 0.1610,
        0.0000, 0.0890, 0.2732, 0.0000, 0.0000, 0.0000, 0.0000, 0.1895, 0.3428,
        0.2576, 0.0000, 0.0000, 0.0000, 0.4500, 0.0000, 0.1270, 0.0106, 0.0602,
        0.0000, 0.2917, 0.0000, 0.1384, 0.0000, 0.0000, 0.1997, 0.0000, 0.0000,
        0.6367, 0.0000, 0.0000, 0.4355, 0.0000, 0.1750, 0.0000, 0.0000, 0.1642,
        0.4077, 0.0078, 0.0000, 0.4429, 0.0000, 0.1886, 0.1078, 0.1750, 0.0000,
        0.0000, 0.0272, 0.0921, 0.0000, 0.0000, 0.1260, 0.6016, 0.0000, 0.0000,
        0.0378], device='cuda:0', dtype=torch.float16)

output_cu :torch.Size([24, 176, 19, 256])

diff max:0.0009765625

==101319== IXPROF is profiling process 101319, command: /usr/local/bin/python3 two_test_pad.py 
==101319== Profiling application: /usr/local/bin/python3 two_test_pad.py 
==101319== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   20.31%  7.8458ms       111  70.683us  58.573us  214.47us  [CUDA memcpy DtoH]
                   12.03%  4.6487ms         1  4.6487ms  4.6487ms  4.6487ms  void ker_input_pad<__half>(__half*, __half*, int, int, int, int)
                   10.96%  4.2349ms         2  2.1175ms  1.8233ms  2.4117ms  void cuinfer::impl::kernel::implConvolution2DTcuKernelFp16Template_N16_3x3_1x1_DB<256u, 256u, 32u, 64u, 64u, 3, 3, true, false, false, 1, 1, 0, 0, __half, __half, float>(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, __half*, float const*, float, float, float, float, __half const*, unsigned int, unsigned int, unsigned int, unsigned int, bool)
                    9.81%  3.7881ms         1  3.7881ms  3.7881ms  3.7881ms  void cudnn::impl::MR::kernel::implConvolution2DTcuKernelHalfTemplateDB<256, 256, 64, 64, 64, 256, true, false, float, __half, __half, float, float, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5> >(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half const*, float const*, int, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5>, int)
                    9.09%  3.5115ms         2  1.7557ms  1.5563ms  1.9552ms  void cuinfer::impl::kernel::NhwcToN16c64hwn16c64<__half>(unsigned int, unsigned int, unsigned int, unsigned int, __half const*, __half*)
                    7.94%  3.0668ms         2  1.5334ms  608.82us  2.4580ms  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6})
                    7.52%  2.9054ms         1  2.9054ms  2.9054ms  2.9054ms  void cudnn::impl::MR::kernel::implConvolution2DKernel<3, 3, 32, float, __half, __half, float, float, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5> >(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half const*, float const*, int, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5>)
                    7.12%  2.7501ms         4  687.53us  9.4260us  2.7216ms  [CUDA memcpy HtoD]
                    3.69%  1.4257ms         2  712.86us  30.300us  1.3954ms  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6})
                    2.73%  1.0549ms         2  527.43us  214.07us  840.80us  void at::native::modern::elementwise_kernel<at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2>)
                    2.06%  798.59us         3  266.19us  12.300us  576.72us  void at::native::modern::elementwise_kernel<at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1>)
                    1.96%  758.80us         2  379.40us  52.553us  706.25us  void cudnn::impl::kernel::IxdnnTransformTensorKernelHalfOpt<64u, 128u, 32u>(unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, short const*, short*)
                    0.87%  336.94us         1  336.94us  336.94us  336.94us  void ker_output_data<__half>(__half*, __half*)
                    0.74%  289.05us         1  289.05us  289.05us  289.05us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> >, at::detail::Array<char*, 3>)
                    0.54%  209.95us         1  209.95us  209.95us  209.95us  void at::native::modern::elementwise_kernel<at::native::AbsFunctor<c10::Half>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<c10::Half>, at::detail::Array<char*, 2>)
                    0.49%  190.97us         1  190.97us  190.97us  190.97us  void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::MaxNanFunctor<c10::Half> >, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::MaxNanFunctor<c10::Half> >, unsigned int, c10::Half, 4>)
                    0.36%  140.70us         1  140.70us  140.70us  140.70us  [CUDA memset]
                    0.16%  62.469us         4  15.617us  10.950us  27.450us  void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})
                    0.14%  57.650us         1  57.650us  57.650us  57.650us  _ZN2at6native12_GLOBAL__N_143distribution_elementwise_grid_stride_kernelIfLi4EZNS0_9templates4cuda20normal_and_transformIN3c104HalfEfLm4EPNS_17CUDAGeneratorImplEZZZNS4_13normal_kernelIS9_EEvRNS_6TensorEddT_ENKUlvE_clEvENKUlvE4_clEvEUlfE_EEvRNS_18TensorIteratorBaseET2_T3_EUlP24curandStatePhilox4_32_10E0_ZNS1_27distribution_nullary_kernelIS7_fLi4ES9_SN_SG_EEvSI_SJ_RKSK_T4_EUlifE_EEviNS_15PhiloxCudaStateET1_SJ_
                    0.12%  48.868us         4  12.217us  11.926us  12.846us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6})
                    0.11%  46.075us         4  11.518us  11.026us  11.726us  void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})
                    0.11%  43.428us         4  10.857us  10.576us  11.550us  void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>)
                    0.10%  41.523us         4  10.380us  9.8500us  10.823us  void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>)
                    0.09%  36.050us         2  18.025us  17.150us  18.900us  void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4>)
                    0.08%  34.299us         2  17.149us  16.973us  17.326us  void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4>)
                    0.08%  32.850us         2  16.425us  16.250us  16.600us  void at::native::index_elementwise_kernel<128, 4, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})
                    0.07%  27.949us         2  13.974us  13.523us  14.426us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6})
                    0.07%  27.879us         2  13.939us  13.276us  14.603us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6})
                    0.06%  26.600us         2  13.300us  13.000us  13.600us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6})
                    0.06%  24.952us         2  12.476us  12.426us  12.526us  void at::cuda::detail::cub::DeviceReduceSingleTileKernel<at::cuda::detail::cub::DeviceReducePolicy<bool, int, int, at::cuda::detail::cub::Sum>::Policy600, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int>(at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int)
                    0.06%  24.945us         2  12.472us  12.146us  12.799us  void at::cuda::detail::cub::DeviceSelectSweepKernel<at::cuda::detail::cub::DispatchSelectIf<at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>::PtxSelectIfPolicyT, at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>(at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, int)
                    0.06%  24.846us         2  12.423us  12.123us  12.723us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6})
                    0.05%  21.629us         2  10.814us  10.726us  10.903us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3>)
                    0.05%  21.376us         2  10.688us  10.576us  10.800us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3>)
                    0.05%  21.299us         2  10.649us  10.373us  10.926us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3>)
                    0.05%  20.323us         2  10.161us  9.8730us  10.450us  void at::cuda::detail::cub::DeviceCompactInitKernel<at::cuda::detail::cub::ScanTileState<int, true>, int*>(at::cuda::detail::cub::ScanTileState<int, true>, int, int*)
                    0.03%  11.826us         1  11.826us  11.826us  11.826us  void ker_weight_pad_trans<__half>(__half*, __half*, int)
      API Calls:   86.30%  5.90723s        65  90.880ms  176.00ns  1.67835s  cuModuleLoadData
                   12.91%  884.02ms         8  110.50ms  1.6800us  883.96ms  cudaFuncGetAttributes
                    0.27%  18.523ms       115  161.07us  112.55us  402.55us  cudaStreamSynchronize
                    0.26%  18.211ms         9  2.0235ms  912.00ns  2.7576ms  cudaSetDevice
                    0.12%  8.8542ms       115  76.993us  2.7740us  2.6090ms  cudaMemcpyAsync
                    0.09%  6.5321ms         2  3.2660ms  2.0644ms  4.4677ms  cudaFree
                    0.00%  482.83us         1  482.83us  482.83us  482.83us  cudaMemsetAsync
                    0.00%  329.36us        70  4.7050us  1.4660us  38.259us  cudaLaunchKernel
                    0.00%  255.19us         8  31.899us  3.7320us  84.031us  cudaMalloc
                    0.00%  228.99us      1072  213.00ns  175.00ns  1.1280us  cudaGetDevice
                    0.00%  59.899us        18  3.3270us  761.00ns  11.270us  cudaGetDeviceProperties
                    0.00%  18.249us        39  467.00ns  211.00ns  3.7290us  cudaStreamIsCapturing
                    0.00%  14.185us        69  205.00ns  54.000ns  871.00ns  cudaGetLastError
                    0.00%  6.4660us         3  2.1550us  96.000ns  5.9200us  cudaGetDeviceCount
                    0.00%  6.1590us        33  186.00ns  45.000ns  4.4700us  cuDevicePrimaryCtxGetState
                    0.00%  4.7770us         2  2.3880us  2.0840us  2.6930us  cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
                    0.00%  3.7810us         6  630.00ns  188.00ns  1.3250us  cudaDeviceGetAttribute
                    0.00%  1.7170us         6  286.00ns  85.000ns  763.00ns  cudaPeekAtLastError
