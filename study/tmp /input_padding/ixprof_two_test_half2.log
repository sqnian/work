nohup: ignoring input
test op 

output_pt:tensor([0.1268, 0.0000, 0.2979, 0.2075, 0.0000, 0.0000, 0.1240, 0.0099, 0.0000,
        0.0629, 0.0126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1216, 0.0000,
        0.0000, 0.4124, 0.3174, 0.2001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.1678, 0.0000, 0.1964, 0.1820, 0.0000, 0.0000, 0.2285, 0.0276, 0.0966,
        0.0000, 0.0000, 0.6958, 0.1216, 0.0000, 0.0997, 0.0000, 0.2795, 0.1609,
        0.0000, 0.0891, 0.2732, 0.0000, 0.0000, 0.0000, 0.0000, 0.1895, 0.3425,
        0.2578, 0.0000, 0.0000, 0.0000, 0.4500, 0.0000, 0.1271, 0.0106, 0.0602,
        0.0000, 0.2917, 0.0000, 0.1383, 0.0000, 0.0000, 0.1998, 0.0000, 0.0000,
        0.6367, 0.0000, 0.0000, 0.4355, 0.0000, 0.1749, 0.0000, 0.0000, 0.1642,
        0.4077, 0.0078, 0.0000, 0.4426, 0.0000, 0.1885, 0.1078, 0.1750, 0.0000,
        0.0000, 0.0273, 0.0922, 0.0000, 0.0000, 0.1260, 0.6016, 0.0000, 0.0000,
        0.0378], device='cuda:0', dtype=torch.float16,
       grad_fn=<SliceBackward0>)

output_pt shape:torch.Size([24, 176, 19, 256])
output_cu:tensor([0.1270, 0.0000, 0.2979, 0.2075, 0.0000, 0.0000, 0.1241, 0.0099, 0.0000,
        0.0628, 0.0126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1216, 0.0000,
        0.0000, 0.4124, 0.3174, 0.2002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.1681, 0.0000, 0.1964, 0.1820, 0.0000, 0.0000, 0.2286, 0.0277, 0.0966,
        0.0000, 0.0000, 0.6958, 0.1216, 0.0000, 0.0996, 0.0000, 0.2793, 0.1610,
        0.0000, 0.0890, 0.2732, 0.0000, 0.0000, 0.0000, 0.0000, 0.1895, 0.3428,
        0.2576, 0.0000, 0.0000, 0.0000, 0.4500, 0.0000, 0.1270, 0.0106, 0.0602,
        0.0000, 0.2917, 0.0000, 0.1384, 0.0000, 0.0000, 0.1997, 0.0000, 0.0000,
        0.6367, 0.0000, 0.0000, 0.4355, 0.0000, 0.1750, 0.0000, 0.0000, 0.1642,
        0.4077, 0.0078, 0.0000, 0.4429, 0.0000, 0.1886, 0.1078, 0.1750, 0.0000,
        0.0000, 0.0272, 0.0921, 0.0000, 0.0000, 0.1260, 0.6016, 0.0000, 0.0000,
        0.0378], device='cuda:0', dtype=torch.float16)

output_cu :torch.Size([24, 176, 19, 256])

diff max:0.0009765625

==108052== IXPROF is profiling process 108052, command: /usr/local/bin/python3 two_test_pad.py 
==108052== Profiling application: /usr/local/bin/python3 two_test_pad.py 
==108052== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   20.17%  7.7686ms       111  69.987us  58.800us  193.22us  [CUDA memcpy DtoH]
                   12.11%  4.6668ms         1  4.6668ms  4.6668ms  4.6668ms  ker_input_pad_half2(__half*, __half*, int, int, int, int)
                   11.08%  4.2701ms         2  2.1351ms  1.8189ms  2.4513ms  void cuinfer::impl::kernel::implConvolution2DTcuKernelFp16Template_N16_3x3_1x1_DB<256u, 256u, 32u, 64u, 64u, 3, 3, true, false, false, 1, 1, 0, 0, __half, __half, float>(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, __half*, float const*, float, float, float, float, __half const*, unsigned int, unsigned int, unsigned int, unsigned int, bool)
                    9.84%  3.7928ms         1  3.7928ms  3.7928ms  3.7928ms  void cudnn::impl::MR::kernel::implConvolution2DTcuKernelHalfTemplateDB<256, 256, 64, 64, 64, 256, true, false, float, __half, __half, float, float, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5> >(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half const*, float const*, int, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5>, int)
                    9.10%  3.5070ms         2  1.7535ms  1.5514ms  1.9556ms  void cuinfer::impl::kernel::NhwcToN16c64hwn16c64<__half>(unsigned int, unsigned int, unsigned int, unsigned int, __half const*, __half*)
                    7.95%  3.0645ms         2  1.5322ms  607.62us  2.4569ms  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6})
                    7.53%  2.9010ms         1  2.9010ms  2.9010ms  2.9010ms  void cudnn::impl::MR::kernel::implConvolution2DKernel<3, 3, 32, float, __half, __half, float, float, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5> >(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half const*, float const*, int, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5>)
                    6.95%  2.6773ms         4  669.31us  9.3960us  2.6487ms  [CUDA memcpy HtoD]
                    3.71%  1.4294ms         2  714.71us  30.526us  1.3989ms  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6})
                    2.74%  1.0553ms         2  527.62us  213.30us  841.95us  void at::native::modern::elementwise_kernel<at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2>)
                    2.07%  799.49us         3  266.49us  13.446us  576.32us  void at::native::modern::elementwise_kernel<at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1>)
                    1.97%  761.92us         2  380.96us  51.900us  710.02us  void cudnn::impl::kernel::IxdnnTransformTensorKernelHalfOpt<64u, 128u, 32u>(unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, short const*, short*)
                    0.87%  337.02us         1  337.02us  337.02us  337.02us  void ker_output_data<__half>(__half*, __half*)
                    0.74%  288.12us         1  288.12us  288.12us  288.12us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> >, at::detail::Array<char*, 3>)
                    0.54%  209.97us         1  209.97us  209.97us  209.97us  void at::native::modern::elementwise_kernel<at::native::AbsFunctor<c10::Half>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<c10::Half>, at::detail::Array<char*, 2>)
                    0.49%  189.17us         1  189.17us  189.17us  189.17us  void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::MaxNanFunctor<c10::Half> >, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::MaxNanFunctor<c10::Half> >, unsigned int, c10::Half, 4>)
                    0.35%  135.42us         1  135.42us  135.42us  135.42us  [CUDA memset]
                    0.16%  62.299us         4  15.574us  10.923us  27.300us  void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})
                    0.14%  57.546us         1  57.546us  57.546us  57.546us  _ZN2at6native12_GLOBAL__N_143distribution_elementwise_grid_stride_kernelIfLi4EZNS0_9templates4cuda20normal_and_transformIN3c104HalfEfLm4EPNS_17CUDAGeneratorImplEZZZNS4_13normal_kernelIS9_EEvRNS_6TensorEddT_ENKUlvE_clEvENKUlvE4_clEvEUlfE_EEvRNS_18TensorIteratorBaseET2_T3_EUlP24curandStatePhilox4_32_10E0_ZNS1_27distribution_nullary_kernelIS7_fLi4ES9_SN_SG_EEvSI_SJ_RKSK_T4_EUlifE_EEviNS_15PhiloxCudaStateET1_SJ_
                    0.12%  48.349us         4  12.087us  11.900us  12.276us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6})
                    0.12%  47.502us         4  11.875us  11.050us  12.326us  void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})
                    0.10%  41.849us         4  10.462us  9.8230us  11.550us  void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>)
                    0.10%  41.018us         4  10.254us  9.5760us  10.823us  void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>)
                    0.09%  35.226us         2  17.613us  17.176us  18.050us  void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4>)
                    0.08%  33.226us         2  16.613us  16.323us  16.903us  void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4>)
                    0.08%  32.976us         2  16.488us  16.426us  16.550us  void at::native::index_elementwise_kernel<128, 4, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})
                    0.07%  30.226us         2  15.113us  14.600us  15.626us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6})
                    0.07%  27.946us         2  13.973us  13.573us  14.373us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6})
                    0.06%  26.426us         2  13.213us  12.750us  13.676us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6})
                    0.06%  25.500us         2  12.750us  12.550us  12.950us  void at::cuda::detail::cub::DeviceSelectSweepKernel<at::cuda::detail::cub::DispatchSelectIf<at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>::PtxSelectIfPolicyT, at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>(at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, int)
                    0.06%  25.450us         2  12.725us  12.700us  12.750us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6})
                    0.06%  24.802us         2  12.401us  12.176us  12.626us  void at::cuda::detail::cub::DeviceReduceSingleTileKernel<at::cuda::detail::cub::DeviceReducePolicy<bool, int, int, at::cuda::detail::cub::Sum>::Policy600, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int>(at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int)
                    0.05%  21.653us         2  10.826us  10.600us  11.053us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3>)
                    0.05%  21.402us         2  10.701us  10.576us  10.826us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3>)
                    0.05%  21.176us         2  10.588us  10.350us  10.826us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3>)
                    0.05%  20.623us         2  10.311us  10.000us  10.623us  void at::cuda::detail::cub::DeviceCompactInitKernel<at::cuda::detail::cub::ScanTileState<int, true>, int*>(at::cuda::detail::cub::ScanTileState<int, true>, int, int*)
                    0.03%  12.973us         1  12.973us  12.973us  12.973us  void ker_weight_pad_trans<__half>(__half*, __half*, int)
      API Calls:   85.85%  5.65084s        65  86.936ms  174.00ns  1.48018s  cuModuleLoadData
                   13.26%  873.09ms         8  109.13ms  1.4000us  873.04ms  cudaFuncGetAttributes
                    0.28%  18.506ms       115  160.92us  138.90us  384.72us  cudaStreamSynchronize
                    0.26%  17.282ms         9  1.9203ms  973.00ns  2.5464ms  cudaSetDevice
                    0.13%  8.5795ms       115  74.604us  2.7620us  2.5363ms  cudaMemcpyAsync
                    0.10%  6.6686ms         2  3.3343ms  2.0957ms  4.5729ms  cudaFree
                    0.08%  5.5913ms         3  1.8638ms  155.13us  4.7481ms  cudaDeviceSynchronize
                    0.00%  487.36us         1  487.36us  487.36us  487.36us  cudaMemsetAsync
                    0.00%  290.52us        70  4.1500us  1.2360us  41.994us  cudaLaunchKernel
                    0.00%  255.40us         8  31.925us  3.7430us  85.064us  cudaMalloc
                    0.00%  219.65us      1072  204.00ns  176.00ns  901.00ns  cudaGetDevice
                    0.00%  56.244us        18  3.1240us  665.00ns  10.727us  cudaGetDeviceProperties
                    0.00%  16.748us        39  429.00ns  208.00ns  2.5930us  cudaStreamIsCapturing
                    0.00%  13.114us        69  190.00ns  55.000ns  879.00ns  cudaGetLastError
                    0.00%  5.5860us        33  169.00ns  41.000ns  3.9300us  cuDevicePrimaryCtxGetState
                    0.00%  5.2550us         3  1.7510us  124.00ns  4.7840us  cudaGetDeviceCount
                    0.00%  4.6890us         2  2.3440us  2.1970us  2.4920us  cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
                    0.00%  3.6210us         6  603.00ns  195.00ns  1.3730us  cudaDeviceGetAttribute
                    0.00%  1.0210us         6  170.00ns  74.000ns  332.00ns  cudaPeekAtLastError
