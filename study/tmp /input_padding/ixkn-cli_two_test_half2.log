nohup: ignoring input
==109250== IXKN-CLI is profiling process 109250, command: /usr/local/bin/python3 two_test_pad.py 
==109250== Some kernel(s) will be replayed on device 0 in order to collect all events/metrics.
==109250== Replaying kernel "_ZN2at6native12_GLOBAL__N_143distribution_elementwise_grid_stride_kernelIfLi4EZNS0_9templates4cuda20normal_and_transformIN3c104HalfEfLm4EPNS_17CUDAGeneratorImplEZZZNS4_13normal_kernelIS9_EEvRNS_6TensorEddT_ENKUlvE_clEvENKUlvE4_clEvEUlfE_EEvRNS_18TensorIteratorBaseET2_T3_EUlP24curandStatePhilox4_32_10E0_ZNS1_27distribution_nullary_kernelIS7_fLi4ES9_SN_SG_EEvSI_SJ_RKSK_T4_EUlifE_EEviNS_15PhiloxCudaStateET1_SJ_" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})" (done)
==109250== Replaying kernel "void cudnn::impl::MR::kernel::implConvolution2DKernel<3, 3, 32, float, __half, __half, float, float, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5> >(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half const*, float const*, int, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5>)" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void cudnn::impl::kernel::IxdnnTransformTensorKernelHalfOpt<64u, 128u, 32u>(unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, short const*, short*)" (done)
==109250== Replaying kernel "void cudnn::impl::kernel::IxdnnTransformTensorKernelHalfOpt<64u, 128u, 32u>(unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, short const*, short*)" (done)
==109250== Replaying kernel "void cudnn::impl::MR::kernel::implConvolution2DTcuKernelHalfTemplateDB<256, 256, 64, 64, 64, 256, true, false, float, __half, __half, float, float, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5> >(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half const*, float const*, int, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5>, int)" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1>)" (done)
==109250== Replaying kernel "ker_input_pad_half2(__half*, __half*, int, int, int, int)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1>)" (done)
==109250== Replaying kernel "void ker_weight_pad_trans<__half>(__half*, __half*, int)" (done)
==109250== Replaying kernel "void cuinfer::impl::kernel::NhwcToN16c64hwn16c64<__half>(unsigned int, unsigned int, unsigned int, unsigned int, __half const*, __half*)" (done)
==109250== Replaying kernel "void cuinfer::impl::kernel::implConvolution2DTcuKernelFp16Template_N16_3x3_1x1_DB<256u, 256u, 32u, 64u, 64u, 3, 3, true, false, false, 1, 1, 0, 0, __half, __half, float>(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, __half*, float const*, float, float, float, float, __half const*, unsigned int, unsigned int, unsigned int, unsigned int, bool)" (done)
==109250== Replaying kernel "void cuinfer::impl::kernel::NhwcToN16c64hwn16c64<__half>(unsigned int, unsigned int, unsigned int, unsigned int, __half const*, __half*)" (done)
==109250== Replaying kernel "void cuinfer::impl::kernel::implConvolution2DTcuKernelFp16Template_N16_3x3_1x1_DB<256u, 256u, 32u, 64u, 64u, 3, 3, true, false, false, 1, 1, 0, 0, __half, __half, float>(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, __half*, float const*, float, float, float, float, __half const*, unsigned int, unsigned int, unsigned int, unsigned int, bool)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1>)" (done)
==109250== Replaying kernel "void ker_output_data<__half>(__half*, __half*)" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3>)" (done)
==109250== Replaying kernel "void at::cuda::detail::cub::DeviceReduceSingleTileKernel<at::cuda::detail::cub::DeviceReducePolicy<bool, int, int, at::cuda::detail::cub::Sum>::Policy600, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int>(at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int)" (done)
==109250== Replaying kernel "void at::cuda::detail::cub::DeviceCompactInitKernel<at::cuda::detail::cub::ScanTileState<int, true>, int*>(at::cuda::detail::cub::ScanTileState<int, true>, int, int*)" (done)
==109250== Replaying kernel "void at::cuda::detail::cub::DeviceSelectSweepKernel<at::cuda::detail::cub::DispatchSelectIf<at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>::PtxSelectIfPolicyT, at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>(at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, int)" (done)
==109250== Replaying kernel "void at::native::index_elementwise_kernel<128, 4, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4>)" (done)
==109250== Replaying kernel "void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4>)" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3>)" (done)
==109250== Replaying kernel "void at::cuda::detail::cub::DeviceReduceSingleTileKernel<at::cuda::detail::cub::DeviceReducePolicy<bool, int, int, at::cuda::detail::cub::Sum>::Policy600, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int>(at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int)" (done)
==109250== Replaying kernel "void at::cuda::detail::cub::DeviceCompactInitKernel<at::cuda::detail::cub::ScanTileState<int, true>, int*>(at::cuda::detail::cub::ScanTileState<int, true>, int, int*)" (done)
==109250== Replaying kernel "void at::cuda::detail::cub::DeviceSelectSweepKernel<at::cuda::detail::cub::DispatchSelectIf<at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>::PtxSelectIfPolicyT, at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>(at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, int)" (done)
==109250== Replaying kernel "void at::native::index_elementwise_kernel<128, 4, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4>)" (done)
==109250== Replaying kernel "void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4>)" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUtest op 

output_pt:tensor([0.3872, 0.0000, 2.2051, 0.0266, 0.0000, 0.0000, 0.4756, 0.0000, 0.1179,
        0.0000, 0.5088, 0.3352, 0.0000, 0.0000, 0.0000, 0.6597, 0.0000, 0.0000,
        0.5264, 0.2415, 0.7769, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        1.8467, 0.0000, 1.7676, 1.1123, 0.0000, 0.0000, 0.2107, 0.0000, 1.6855,
        0.4846, 0.1289, 1.4258, 1.0225, 0.6416, 0.0000, 0.7568, 1.4385, 0.5581,
        0.0000, 1.4053, 0.7100, 0.6973, 0.9985, 0.0000, 0.0000, 0.0000, 2.2383,
        0.0000, 0.4282, 0.0000, 0.3169, 1.4023, 0.0000, 0.0000, 0.0540, 0.0000,
        0.0000, 2.5918, 0.0000, 0.9399, 0.4263, 0.0000, 1.1465, 0.2373, 0.0000,
        2.9902, 0.0000, 0.1724, 0.6636, 0.0000, 1.2393, 0.0000, 0.7974, 0.0000,
        1.4219, 1.2900, 0.0000, 2.2266, 0.0000, 0.5278, 0.0000, 0.5659, 0.8369,
        0.0000, 0.0000, 0.0000, 0.0000, 1.3389, 0.7183, 2.3027, 0.0000, 0.0000,
        0.3708], device='cuda:0', dtype=torch.float16,
       grad_fn=<SliceBackward0>)

output_pt shape:torch.Size([24, 176, 19, 256])
output_cu:tensor([0.1270, 0.0000, 0.2979, 0.2075, 0.0000, 0.0000, 0.1241, 0.0099, 0.0000,
        0.0628, 0.0126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1216, 0.0000,
        0.0000, 0.4124, 0.3174, 0.2002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.1681, 0.0000, 0.1964, 0.1820, 0.0000, 0.0000, 0.2286, 0.0277, 0.0966,
        0.0000, 0.0000, 0.6958, 0.1216, 0.0000, 0.0996, 0.0000, 0.2793, 0.1610,
        0.0000, 0.0890, 0.2732, 0.0000, 0.0000, 0.0000, 0.0000, 0.1895, 0.3428,
        0.2576, 0.0000, 0.0000, 0.0000, 0.4500, 0.0000, 0.1270, 0.0106, 0.0602,
        0.0000, 0.2917, 0.0000, 0.1384, 0.0000, 0.0000, 0.1997, 0.0000, 0.0000,
        0.6367, 0.0000, 0.0000, 0.4355, 0.0000, 0.1750, 0.0000, 0.0000, 0.1642,
        0.4077, 0.0078, 0.0000, 0.4429, 0.0000, 0.1886, 0.1078, 0.1750, 0.0000,
        0.0000, 0.0272, 0.0921, 0.0000, 0.0000, 0.1260, 0.6016, 0.0000, 0.0000,
        0.0378], device='cuda:0', dtype=torch.float16)

output_cu :torch.Size([24, 176, 19, 256])

diff max:2.919921875

naryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6})" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> >, at::detail::Array<char*, 3>)" (done)
==109250== Replaying kernel "void at::native::modern::elementwise_kernel<at::native::AbsFunctor<c10::Half>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<c10::Half>, at::detail::Array<char*, 2>)" (done)
==109250== Replaying kernel "void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::MaxNanFunctor<c10::Half> >, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::MaxNanFunctor<c10::Half> >, unsigned int, c10::Half, 4>)" (done)
==109250== Profiling application: /usr/local/bin/python3 two_test_pad.py 
==109250== Profiling result:
==109250== Section Results:
  Kernel: _ZN2at6native12_GLOBAL__N_143distribution_elementwise_grid_stride_kernelIfLi4EZNS0_9templates4cuda20normal_and_transformIN3c104HalfEfLm4EPNS_17CUDAGeneratorImplEZZZNS4_13normal_kernelIS9_EEvRNS_6TensorEddT_ENKUlvE_clEvENKUlvE4_clEvEUlfE_EEvRNS_18TensorIteratorBaseET2_T3_EUlP24curandStatePhilox4_32_10E0_ZNS1_27distribution_nullary_kernelIS7_fLi4ES9_SN_SG_EEvSI_SJ_RKSK_T4_EUlifE_EEviNS_15PhiloxCudaStateET1_SJ_, Context 1, Stream 1, correlationId 739
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             57573 Ns             
    Elapsed Cycles                                                                     3514880 Cycles         
    SPP Active Cycles                                                                  2982951 Cycles         
    Scheduler(NS) Issue Cycles                                                         2081537 Cycles         
    ALU Active Cycles                                                                  2626912 Cycles         
    LSU Active Cycles                                                                   306904 Cycles         
    L1 Vector Cache Active Cycles                                                            0 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: High Compute Throughput --- Compute is more heavily utilized than Memory: Look at the [Compute Workload
Analysis] section to see what the compute pipelines are spending their time doing. Also, consider whether
any computation is redundant and could be reduced or moved to look-up tables.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.668719 Inst/Cycle     
    Executed Instructions                                                              2341180 Instructions   
    Single-Lane Memory Instructions                                                      15360 Instructions   
    Single-Lane ALU Instructions                                                        345600 Instructions   
    Single-Lane Control Flow Instructions                                                92160 Instructions   
    Multi-Lane Memory Instructions                                                       21300 Instructions   
     -Multi-Lane Global Memory Instructions                                              21300 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                        1866760 Instructions   
     -Multi-Lane ALU Integer Instructions                                              1343800 Instructions   
     -Multi-Lane ALU Half Float Instructions                                             21300 Instructions   
     -Multi-Lane ALU Single Float Instructions                                          501660 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                           225280 Instructions   
     +SFU Instructions                                                                   39220 Instructions   
     +BFU Instructions                                                                 1602260 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                      [640, 1, 1]                
    Block Size                                                                     [256, 1, 1]                
    Launched Warps                                                                        2560 Warps          
    Threads                                                                             163840 Threads        
    Scalar Register                                                                         40 Registers/Warp 
    Vector Register                                                                         24 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Tail Effect --- A wave of thread blocks is defined as the maximum number of blocks that can be executed
in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and
the theoretical occupancy of the kernel. This kernel launch results in 1.0 full waves and a partial wave of
128 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial
wave may account for up to 50.0% of the total kernel runtime. Try launching a grid with no partial wave.
The overall impact of this tail effect also lessens with the number of full waves executed for a grid.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       8.73 %              
    LSU--Global Memory Utilization                                                        8.73 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                          0.0 B/s            
    Global Memory Store Throughput                                                         0.0 B/s            
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  0.0 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                        50.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         90.38 Warps          
    Achieved Occupancy                                                                   70.61 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  42 Blocks         
    Block Limit SRF Per CU                                                                  51 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                32 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                          1.25                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (70.61%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}), Context 1, Stream 1, correlationId 807
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12200 Ns             
    Elapsed Cycles                                                                      687872 Cycles         
    SPP Active Cycles                                                                    41005 Cycles         
    Scheduler(NS) Issue Cycles                                                            2068 Cycles         
    ALU Active Cycles                                                                     3212 Cycles         
    LSU Active Cycles                                                                    12746 Cycles         
    L1 Vector Cache Active Cycles                                                         3810 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.02 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.003122 Inst/Cycle     
    Executed Instructions                                                                 2068 Instructions   
    Single-Lane Memory Instructions                                                        112 Instructions   
    Single-Lane ALU Instructions                                                           456 Instructions   
    Single-Lane Control Flow Instructions                                                  368 Instructions   
    Multi-Lane Memory Instructions                                                         144 Instructions   
     -Multi-Lane Global Memory Instructions                                                144 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            988 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  952 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                36 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                               72 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     916 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [5, 1, 1]                
    Block Size                                                                     [512, 1, 1]                
    Launched Warps                                                                          40 Warps          
    Threads                                                                               2560 Threads        
    Scalar Register                                                                         16 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 5 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       1.85 %              
    LSU--Global Memory Utilization                                                        1.85 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         1.06 GB/s           
    Global Memory Store Throughput                                                      720.41 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.55 %              
    L1 Vector Cache Load Hit Rate                                                        15.62 %              
    L1 Vector Cache Store Hit Rate                                                        50.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         13.06 Warps          
    Achieved Occupancy                                                                    10.2 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  46 Blocks         
    Block Limit SRF Per CU                                                                  64 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                16 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                          0.02                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (10.2%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}), Context 1, Stream 1, correlationId 824
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             11300 Ns             
    Elapsed Cycles                                                                      667136 Cycles         
    SPP Active Cycles                                                                     4433 Cycles         
    Scheduler(NS) Issue Cycles                                                             276 Cycles         
    ALU Active Cycles                                                                      398 Cycles         
    LSU Active Cycles                                                                     1425 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000417 Inst/Cycle     
    Executed Instructions                                                                  276 Instructions   
    Single-Lane Memory Instructions                                                         16 Instructions   
    Single-Lane ALU Instructions                                                            72 Instructions   
    Single-Lane Control Flow Instructions                                                   48 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            124 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  120 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 4 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                8 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     116 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [512, 1, 1]                
    Launched Warps                                                                           8 Warps          
    Threads                                                                                512 Threads        
    Scalar Register                                                                         16 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.21 %              
    LSU--Global Memory Utilization                                                        0.21 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       129.63 MB/s           
    Global Memory Store Throughput                                                       86.42 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  0.2 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                      0.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           9.3 Warps          
    Achieved Occupancy                                                                    7.27 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  46 Blocks         
    Block Limit SRF Per CU                                                                  64 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                16 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (7.27%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}), Context 1, Stream 1, correlationId 840
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             27423 Ns             
    Elapsed Cycles                                                                     1702784 Cycles         
    SPP Active Cycles                                                                  1224488 Cycles         
    Scheduler(NS) Issue Cycles                                                          452721 Cycles         
    ALU Active Cycles                                                                   598495 Cycles         
    LSU Active Cycles                                                                   699553 Cycles         
    L1 Vector Cache Active Cycles                                                       235690 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Latency Issue --- This kernel exhibits low compute throughput and memory bandwidth utilization relative
to the peak performance of this device.Achieved compute throughput and/or memory bandwidth below 60% of
peak typically indicate latency issues. Look at [Scheduler Statistics] and [Warp State Statistics] for
potential reasons.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.295710 Inst/Cycle     
    Executed Instructions                                                               516096 Instructions   
    Single-Lane Memory Instructions                                                      27648 Instructions   
    Single-Lane ALU Instructions                                                        110592 Instructions   
    Single-Lane Control Flow Instructions                                                92160 Instructions   
    Multi-Lane Memory Instructions                                                       36864 Instructions   
     -Multi-Lane Global Memory Instructions                                              36864 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                         248832 Instructions   
     -Multi-Lane ALU Integer Instructions                                               239616 Instructions   
     -Multi-Lane ALU Half Float Instructions                                              9216 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                            18432 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                  230400 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                     [1152, 1, 1]                
    Block Size                                                                     [512, 1, 1]                
    Launched Warps                                                                        9216 Warps          
    Threads                                                                             589824 Threads        
    Scalar Register                                                                         16 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Tail Effect --- A wave of thread blocks is defined as the maximum number of blocks that can be executed
in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and
the theoretical occupancy of the kernel. This kernel launch results in 4.0 full waves and a partial wave of
128 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial
wave may account for up to 20.0% of the total kernel runtime. Try launching a grid with no partial wave.
The overall impact of this tail effect also lessens with the number of full waves executed for a grid.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      41.08 %              
    LSU--Global Memory Utilization                                                       41.08 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       120.19 GB/s           
    Global Memory Store Throughput                                                       80.12 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                13.84 %              
    L1 Vector Cache Load Hit Rate                                                        16.65 %              
    L1 Vector Cache Store Hit Rate                                                        50.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         100.5 Warps          
    Achieved Occupancy                                                                   78.52 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  46 Blocks         
    Block Limit SRF Per CU                                                                  64 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                16 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           4.5                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (78.52%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}), Context 1, Stream 1, correlationId 856
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             10623 Ns             
    Elapsed Cycles                                                                      621056 Cycles         
    SPP Active Cycles                                                                     7460 Cycles         
    Scheduler(NS) Issue Cycles                                                             276 Cycles         
    ALU Active Cycles                                                                      397 Cycles         
    LSU Active Cycles                                                                     1399 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000417 Inst/Cycle     
    Executed Instructions                                                                  276 Instructions   
    Single-Lane Memory Instructions                                                         16 Instructions   
    Single-Lane ALU Instructions                                                            72 Instructions   
    Single-Lane Control Flow Instructions                                                   48 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            124 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  120 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 4 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                8 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     116 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [512, 1, 1]                
    Launched Warps                                                                           8 Warps          
    Threads                                                                                512 Threads        
    Scalar Register                                                                         16 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.23 %              
    LSU--Global Memory Utilization                                                        0.23 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       137.89 MB/s           
    Global Memory Store Throughput                                                       91.93 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.22 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           9.3 Warps          
    Achieved Occupancy                                                                    7.27 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  46 Blocks         
    Block Limit SRF Per CU                                                                  64 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                16 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (7.27%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void cudnn::impl::MR::kernel::implConvolution2DKernel<3, 3, 32, float, __half, __half, float, float, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5> >(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half const*, float const*, int, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5>), Context 1, Stream 1, correlationId 876
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            2901.9 Us             
    Elapsed Cycles                                                                   185657600 Cycles         
    SPP Active Cycles                                                                182699092 Cycles         
    Scheduler(NS) Issue Cycles                                                       117650943 Cycles         
    ALU Active Cycles                                                                150647268 Cycles         
    LSU Active Cycles                                                                 87321355 Cycles         
    L1 Vector Cache Active Cycles                                                     34490347 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Compute Workload Analysis section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.722209 Inst/Cycle     
    Executed Instructions                                                            134159265 Instructions   
    Single-Lane Memory Instructions                                                     290080 Instructions   
    Single-Lane ALU Instructions                                                      25982240 Instructions   
    Single-Lane Control Flow Instructions                                              4174440 Instructions   
    Multi-Lane Memory Instructions                                                    12483135 Instructions   
     -Multi-Lane Global Memory Instructions                                            1542975 Instructions   
     -Multi-Lane Shared Memory Instructions                                           10940160 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                       91229370 Instructions   
     -Multi-Lane ALU Integer Instructions                                             45532715 Instructions   
     -Multi-Lane ALU Half Float Instructions                                           1325440 Instructions   
     -Multi-Lane ALU Single Float Instructions                                        44371215 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                          7208590 Instructions   
     +SFU Instructions                                                                  124320 Instructions   
     +BFU Instructions                                                                83896460 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                     [1295, 1, 1]                
    Block Size                                                                     [32, 32, 1]                
    Launched Warps                                                                       20720 Warps          
    Threads                                                                            1326080 Threads        
    Scalar Register                                                                         96 Registers/Warp 
    Vector Register                                                                        117 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                 65536 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      47.03 %              
    LSU--Global Memory Utilization                                                       11.39 %              
    LSU--Shared Memory Utilization                                                        36.5 %              
    Global Memory Load Throughput                                                         9.39 GB/s           
    Global Memory Store Throughput                                                       211.2 GB/s           
    Shared Memory Load Throughput                                                        871.6 GB/s           
    Shared Memory Store Throughput                                                       27.24 GB/s           
    Shared Store Bank Conflict                                                          331520 Conflicts      
    Shared Load Bank Conflict                                                         10608640 Conflicts      
    L1VK Unit Utilization                                                                18.58 %              
    L1 Vector Cache Load Hit Rate                                                        87.99 %              
    L1 Vector Cache Store Hit Rate                                                       65.09 %              
    L2 Cache Hit Rate                                                                    29.17 %              
    L2 Cache Access to Memory                                                          2693210 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         31.86 Warps          
    Achieved Occupancy                                                                   24.89 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                   2 Blocks         
    Block Limit SRF Per CU                                                                   5 Blocks         
    Block Limit Shared Mem Per CU                                                            2 Blocks         
    Block Limit Warps Per CU                                                                 8 Blocks         
    Theoretical Active Warps per CU                                                         32 Warps          
    Theoretical Occupancy                                                                 25.0 %              
    Waves Per CU                                                                         40.47                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (25.0%) is limited by the number of required
VRF.This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 885
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                           2456.48 Us             
    Elapsed Cycles                                                                   157054592 Cycles         
    SPP Active Cycles                                                                156548608 Cycles         
    Scheduler(NS) Issue Cycles                                                       103568071 Cycles         
    ALU Active Cycles                                                                151652082 Cycles         
    LSU Active Cycles                                                                135160143 Cycles         
    L1 Vector Cache Active Cycles                                                     38978263 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Compute Workload Analysis section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.821424 Inst/Cycle     
    Executed Instructions                                                            128892816 Instructions   
    Single-Lane Memory Instructions                                                    4638816 Instructions   
    Single-Lane ALU Instructions                                                      25182144 Instructions   
    Single-Lane Control Flow Instructions                                              6958224 Instructions   
    Multi-Lane Memory Instructions                                                     3976128 Instructions   
     -Multi-Lane Global Memory Instructions                                            3976128 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                       88137504 Instructions   
     -Multi-Lane ALU Integer Instructions                                             82836000 Instructions   
     -Multi-Lane ALU Half Float Instructions                                           1325376 Instructions   
     -Multi-Lane ALU Single Float Instructions                                         3976128 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                         19880640 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                68256864 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                   [165672, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                      331344 Warps          
    Threads                                                                           21206016 Threads        
    Scalar Register                                                                         96 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      86.06 %              
    LSU--Global Memory Utilization                                                       86.06 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        96.48 GB/s           
    Global Memory Store Throughput                                                      128.64 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                24.82 %              
    L1 Vector Cache Load Hit Rate                                                        33.26 %              
    L1 Vector Cache Store Hit Rate                                                       100.0 %              
    L2 Cache Hit Rate                                                                     50.0 %              
    L2 Cache Access to Memory                                                          5301504 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         78.96 Warps          
    Achieved Occupancy                                                                   61.69 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  42 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         84 Warps          
    Theoretical Occupancy                                                                65.62 %              
    Waves Per CU                                                                        246.54                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (65.62%) is limited by the number of
required SRF.

  Kernel: void at::native::modern::elementwise_kernel<at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 906
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            840950 Ns             
    Elapsed Cycles                                                                    53557760 Cycles         
    SPP Active Cycles                                                                 53074801 Cycles         
    Scheduler(NS) Issue Cycles                                                        40976926 Cycles         
    ALU Active Cycles                                                                 43389527 Cycles         
    LSU Active Cycles                                                                 52149306 Cycles         
    L1 Vector Cache Active Cycles                                                     13240239 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.946115 Inst/Cycle     
    Executed Instructions                                                             50695632 Instructions   
    Single-Lane Memory Instructions                                                     662688 Instructions   
    Single-Lane ALU Instructions                                                      18886608 Instructions   
    Single-Lane Control Flow Instructions                                              2982096 Instructions   
    Multi-Lane Memory Instructions                                                     2650752 Instructions   
     -Multi-Lane Global Memory Instructions                                            2650752 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                       25513488 Instructions   
     -Multi-Lane ALU Integer Instructions                                             16235856 Instructions   
     -Multi-Lane ALU Half Float Instructions                                           1325376 Instructions   
     -Multi-Lane ALU Single Float Instructions                                         7952256 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                25513488 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                   [165672, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                      331344 Warps          
    Threads                                                                           21206016 Threads        
    Scalar Register                                                                         20 Registers/Warp 
    Vector Register                                                                          8 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      97.37 %              
    LSU--Global Memory Utilization                                                       97.37 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       187.88 GB/s           
    Global Memory Store Throughput                                                      375.76 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                24.72 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                        50.0 %              
    L2 Cache Hit Rate                                                                      0.0 %              
    L2 Cache Access to Memory                                                          5301424 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         36.56 Warps          
    Achieved Occupancy                                                                   28.56 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                        161.79                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (28.56%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void cudnn::impl::kernel::IxdnnTransformTensorKernelHalfOpt<64u, 128u, 32u>(unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, short const*, short*), Context 1, Stream 1, correlationId 927
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            710100 Ns             
    Elapsed Cycles                                                                    44695424 Cycles         
    SPP Active Cycles                                                                 44322693 Cycles         
    Scheduler(NS) Issue Cycles                                                        22217885 Cycles         
    ALU Active Cycles                                                                 23788317 Cycles         
    LSU Active Cycles                                                                 43930779 Cycles         
    L1 Vector Cache Active Cycles                                                     11006403 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.587499 Inst/Cycle     
    Executed Instructions                                                             26376192 Instructions   
    Single-Lane Memory Instructions                                                     331776 Instructions   
    Single-Lane ALU Instructions                                                      10782720 Instructions   
    Single-Lane Control Flow Instructions                                               995328 Instructions   
    Multi-Lane Memory Instructions                                                     2654208 Instructions   
     -Multi-Lane Global Memory Instructions                                            1327104 Instructions   
     -Multi-Lane Shared Memory Instructions                                            1327104 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                       11612160 Instructions   
     -Multi-Lane ALU Integer Instructions                                             11612160 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                           165888 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                11446272 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                     [4, 108, 24]                
    Block Size                                                                      [64, 1, 8]                
    Launched Warps                                                                       82944 Warps          
    Threads                                                                            5308416 Threads        
    Scalar Register                                                                         34 Registers/Warp 
    Vector Register                                                                         14 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                 16384 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      98.29 %              
    LSU--Global Memory Utilization                                                       97.18 %              
    LSU--Shared Memory Utilization                                                        57.7 %              
    Global Memory Load Throughput                                                       431.37 GB/s           
    Global Memory Store Throughput                                                       222.5 GB/s           
    Shared Memory Load Throughput                                                       222.53 GB/s           
    Shared Memory Store Throughput                                                      222.53 GB/s           
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                24.63 %              
    L1 Vector Cache Load Hit Rate                                                        39.03 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                     10.6 %              
    L2 Cache Access to Memory                                                          5301962 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         58.19 Warps          
    Achieved Occupancy                                                                   45.46 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  36 Blocks         
    Block Limit SRF Per CU                                                                  30 Blocks         
    Block Limit Shared Mem Per CU                                                            8 Blocks         
    Block Limit Warps Per CU                                                                16 Blocks         
    Theoretical Active Warps per CU                                                         64 Warps          
    Theoretical Occupancy                                                                 50.0 %              
    Waves Per CU                                                                          81.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (50.0%) is limited by the required amount of
shared memory.

  Kernel: void cudnn::impl::kernel::IxdnnTransformTensorKernelHalfOpt<64u, 128u, 32u>(unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, short const*, short*), Context 1, Stream 1, correlationId 952
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             51800 Ns             
    Elapsed Cycles                                                                     2888192 Cycles         
    SPP Active Cycles                                                                  2356289 Cycles         
    Scheduler(NS) Issue Cycles                                                         1271977 Cycles         
    ALU Active Cycles                                                                  1257725 Cycles         
    LSU Active Cycles                                                                  1178915 Cycles         
    L1 Vector Cache Active Cycles                                                       432107 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Latency Issue --- This kernel exhibits low compute throughput and memory bandwidth utilization relative
to the peak performance of this device.Achieved compute throughput and/or memory bandwidth below 60% of
peak typically indicate latency issues. Look at [Scheduler Statistics] and [Warp State Statistics] for
potential reasons.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.564579 Inst/Cycle     
    Executed Instructions                                                              1609728 Instructions   
    Single-Lane Memory Instructions                                                      26624 Instructions   
    Single-Lane ALU Instructions                                                        690176 Instructions   
    Single-Lane Control Flow Instructions                                                49152 Instructions   
    Multi-Lane Memory Instructions                                                      180224 Instructions   
     -Multi-Lane Global Memory Instructions                                              90112 Instructions   
     -Multi-Lane Shared Memory Instructions                                              90112 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                         663552 Instructions   
     -Multi-Lane ALU Integer Instructions                                               663552 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                            10240 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                  653312 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                      [4, 1, 256]                
    Block Size                                                                      [64, 1, 8]                
    Launched Warps                                                                        8192 Warps          
    Threads                                                                             524288 Threads        
    Scalar Register                                                                         34 Registers/Warp 
    Vector Register                                                                         14 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                 16384 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      40.82 %              
    LSU--Global Memory Utilization                                                       26.89 %              
    LSU--Shared Memory Utilization                                                        23.2 %              
    Global Memory Load Throughput                                                        51.84 GB/s           
    Global Memory Store Throughput                                                       21.21 GB/s           
    Shared Memory Load Throughput                                                        28.28 GB/s           
    Shared Memory Store Throughput                                                       37.71 GB/s           
    Shared Store Bank Conflict                                                           24576 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                14.96 %              
    L1 Vector Cache Load Hit Rate                                                        59.17 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         32.38 Warps          
    Achieved Occupancy                                                                   25.29 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  36 Blocks         
    Block Limit SRF Per CU                                                                  30 Blocks         
    Block Limit Shared Mem Per CU                                                            8 Blocks         
    Block Limit Warps Per CU                                                                16 Blocks         
    Theoretical Active Warps per CU                                                         64 Warps          
    Theoretical Occupancy                                                                 50.0 %              
    Waves Per CU                                                                           8.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (50.0%) is limited by the required amount of
shared memory.The difference between calculated theoretical (50.0%) and measured achieved occupancy
(25.29%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void cudnn::impl::MR::kernel::implConvolution2DTcuKernelHalfTemplateDB<256, 256, 64, 64, 64, 256, true, false, float, __half, __half, float, float, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5> >(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half const*, float const*, int, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5>, int), Context 1, Stream 1, correlationId 971
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                           3789.15 Us             
    Elapsed Cycles                                                                   242568704 Cycles         
    SPP Active Cycles                                                                237474504 Cycles         
    Scheduler(NS) Issue Cycles                                                        92103320 Cycles         
    ALU Active Cycles                                                                181424481 Cycles         
    LSU Active Cycles                                                                212366584 Cycles         
    L1 Vector Cache Active Cycles                                                     46711156 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.428455 Inst/Cycle     
    Executed Instructions                                                            103978080 Instructions   
    Single-Lane Memory Instructions                                                      70336 Instructions   
    Single-Lane ALU Instructions                                                      38543616 Instructions   
    Single-Lane Control Flow Instructions                                              3275136 Instructions   
    Multi-Lane Memory Instructions                                                    16237056 Instructions   
     -Multi-Lane Global Memory Instructions                                            3214848 Instructions   
     -Multi-Lane Shared Memory Instructions                                           13022208 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                       45851936 Instructions   
     -Multi-Lane ALU Integer Instructions                                             33554208 Instructions   
     -Multi-Lane ALU Half Float Instructions                                            321024 Instructions   
     -Multi-Lane ALU Single Float Instructions                                        11976704 Instructions   
     +Multi-Lane Matrix Instructions                                                  11575296 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                          3461536 Instructions   
     +SFU Instructions                                                                   25120 Instructions   
     +BFU Instructions                                                                30789984 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                      [314, 1, 1]                
    Block Size                                                                    [1024, 1, 2]                
    Launched Warps                                                                       10048 Warps          
    Threads                                                                             643072 Threads        
    Scalar Register                                                                         78 Registers/Warp 
    Vector Register                                                                        104 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                131072 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      87.55 %              
    LSU--Global Memory Utilization                                                       67.48 %              
    LSU--Shared Memory Utilization                                                       69.18 %              
    Global Memory Load Throughput                                                       181.94 GB/s           
    Global Memory Store Throughput                                                        40.4 GB/s           
    Shared Memory Load Throughput                                                       728.33 GB/s           
    Shared Memory Store Throughput                                                      182.08 GB/s           
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                19.26 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                       57.79 %              
    L2 Cache Hit Rate                                                                    89.34 %              
    L2 Cache Access to Memory                                                          3473392 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          30.7 Warps          
    Achieved Occupancy                                                                   23.98 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                   1 Blocks         
    Block Limit SRF Per CU                                                                   3 Blocks         
    Block Limit Shared Mem Per CU                                                            1 Blocks         
    Block Limit Warps Per CU                                                                 4 Blocks         
    Theoretical Active Warps per CU                                                         32 Warps          
    Theoretical Occupancy                                                                 25.0 %              
    Waves Per CU                                                                         19.62                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (25.0%) is limited by the number of required
VRF.This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 978
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            609650 Ns             
    Elapsed Cycles                                                                    38996480 Cycles         
    SPP Active Cycles                                                                 38381343 Cycles         
    Scheduler(NS) Issue Cycles                                                        25108140 Cycles         
    ALU Active Cycles                                                                 36766814 Cycles         
    LSU Active Cycles                                                                 32671320 Cycles         
    L1 Vector Cache Active Cycles                                                      9462601 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Compute Workload Analysis section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.803449 Inst/Cycle     
    Executed Instructions                                                             31219584 Instructions   
    Single-Lane Memory Instructions                                                    1123584 Instructions   
    Single-Lane ALU Instructions                                                       6099456 Instructions   
    Single-Lane Control Flow Instructions                                              1685376 Instructions   
    Multi-Lane Memory Instructions                                                      963072 Instructions   
     -Multi-Lane Global Memory Instructions                                             963072 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                       21348096 Instructions   
     -Multi-Lane ALU Integer Instructions                                             20064000 Instructions   
     -Multi-Lane ALU Half Float Instructions                                            321024 Instructions   
     -Multi-Lane ALU Single Float Instructions                                          963072 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                          4815360 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                16532736 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                    [40128, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                       80256 Warps          
    Threads                                                                            5136384 Threads        
    Scalar Register                                                                         96 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      83.78 %              
    LSU--Global Memory Utilization                                                       83.78 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        94.16 GB/s           
    Global Memory Store Throughput                                                      125.54 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                24.27 %              
    L1 Vector Cache Load Hit Rate                                                        33.11 %              
    L1 Vector Cache Store Hit Rate                                                       100.0 %              
    L2 Cache Hit Rate                                                                     50.0 %              
    L2 Cache Access to Memory                                                          1284096 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         78.47 Warps          
    Achieved Occupancy                                                                   61.31 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  42 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         84 Warps          
    Theoretical Occupancy                                                                65.62 %              
    Waves Per CU                                                                         59.71                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (65.62%) is limited by the number of
required SRF.

  Kernel: void at::native::modern::elementwise_kernel<at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 994
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            213500 Ns             
    Elapsed Cycles                                                                    13385216 Cycles         
    SPP Active Cycles                                                                 12901747 Cycles         
    Scheduler(NS) Issue Cycles                                                         9879480 Cycles         
    ALU Active Cycles                                                                 10450461 Cycles         
    LSU Active Cycles                                                                 12520595 Cycles         
    L1 Vector Cache Active Cycles                                                      3195430 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.916746 Inst/Cycle     
    Executed Instructions                                                             12279168 Instructions   
    Single-Lane Memory Instructions                                                     160512 Instructions   
    Single-Lane ALU Instructions                                                       4574592 Instructions   
    Single-Lane Control Flow Instructions                                               722304 Instructions   
    Multi-Lane Memory Instructions                                                      642048 Instructions   
     -Multi-Lane Global Memory Instructions                                             642048 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                        6179712 Instructions   
     -Multi-Lane ALU Integer Instructions                                              3932544 Instructions   
     -Multi-Lane ALU Half Float Instructions                                            321024 Instructions   
     -Multi-Lane ALU Single Float Instructions                                         1926144 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                 6179712 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                    [40128, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                       80256 Warps          
    Threads                                                                            5136384 Threads        
    Scalar Register                                                                         20 Registers/Warp 
    Vector Register                                                                          8 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      93.54 %              
    LSU--Global Memory Utilization                                                       93.54 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       179.25 GB/s           
    Global Memory Store Throughput                                                      358.49 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                23.87 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                       49.99 %              
    L2 Cache Hit Rate                                                                     0.01 %              
    L2 Cache Access to Memory                                                          1284000 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         37.43 Warps          
    Achieved Occupancy                                                                   29.24 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                         39.19                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (29.24%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 1007
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                           1394.65 Us             
    Elapsed Cycles                                                                    88997888 Cycles         
    SPP Active Cycles                                                                 87717685 Cycles         
    Scheduler(NS) Issue Cycles                                                        22888700 Cycles         
    ALU Active Cycles                                                                 37293078 Cycles         
    LSU Active Cycles                                                                 87054243 Cycles         
    L1 Vector Cache Active Cycles                                                     21811786 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.277188 Inst/Cycle     
    Executed Instructions                                                             24558336 Instructions   
    Single-Lane Memory Instructions                                                     882816 Instructions   
    Single-Lane ALU Instructions                                                       5778432 Instructions   
    Single-Lane Control Flow Instructions                                              1685376 Instructions   
    Multi-Lane Memory Instructions                                                      642048 Instructions   
     -Multi-Lane Global Memory Instructions                                             642048 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                       15569664 Instructions   
     -Multi-Lane ALU Integer Instructions                                             15569664 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                          3852288 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                11717376 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                    [40128, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                       80256 Warps          
    Threads                                                                            5136384 Threads        
    Scalar Register                                                                         92 Registers/Warp 
    Vector Register                                                                          9 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      97.82 %              
    LSU--Global Memory Utilization                                                       97.82 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       878.08 GB/s           
    Global Memory Store Throughput                                                       54.88 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                24.51 %              
    L1 Vector Cache Load Hit Rate                                                        14.25 %              
    L1 Vector Cache Store Hit Rate                                                       50.57 %              
    L2 Cache Hit Rate                                                                    92.93 %              
    L2 Cache Access to Memory                                                          1284304 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          78.6 Warps          
    Achieved Occupancy                                                                   61.41 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  44 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         88 Warps          
    Theoretical Occupancy                                                                68.75 %              
    Waves Per CU                                                                          57.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (68.75%) is limited by the number of
required SRF.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 1021
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             31250 Ns             
    Elapsed Cycles                                                                     1843328 Cycles         
    SPP Active Cycles                                                                  1360108 Cycles         
    Scheduler(NS) Issue Cycles                                                          590325 Cycles         
    ALU Active Cycles                                                                   850651 Cycles         
    LSU Active Cycles                                                                   671045 Cycles         
    L1 Vector Cache Active Cycles                                                       225706 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Latency Issue --- This kernel exhibits low compute throughput and memory bandwidth utilization relative
to the peak performance of this device.Achieved compute throughput and/or memory bandwidth below 60% of
peak typically indicate latency issues. Look at [Scheduler Statistics] and [Warp State Statistics] for
potential reasons.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.378947 Inst/Cycle     
    Executed Instructions                                                               705024 Instructions   
    Single-Lane Memory Instructions                                                      25344 Instructions   
    Single-Lane ALU Instructions                                                        165888 Instructions   
    Single-Lane Control Flow Instructions                                                48384 Instructions   
    Multi-Lane Memory Instructions                                                       18432 Instructions   
     -Multi-Lane Global Memory Instructions                                              18432 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                         446976 Instructions   
     -Multi-Lane ALU Integer Instructions                                               446976 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                           110592 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                  336384 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                     [1152, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                        2304 Warps          
    Threads                                                                             147456 Threads        
    Scalar Register                                                                         92 Registers/Warp 
    Vector Register                                                                          9 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Tail Effect --- A wave of thread blocks is defined as the maximum number of blocks that can be executed
in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and
the theoretical occupancy of the kernel. This kernel launch results in 1.0 full waves and a partial wave of
449 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial
wave may account for up to 50.0% of the total kernel runtime. Try launching a grid with no partial wave.
The overall impact of this tail effect also lessens with the number of full waves executed for a grid.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       36.4 %              
    LSU--Global Memory Utilization                                                        36.4 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       316.41 GB/s           
    Global Memory Store Throughput                                                       70.31 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                12.24 %              
    L1 Vector Cache Load Hit Rate                                                         2.91 %              
    L1 Vector Cache Store Hit Rate                                                       50.04 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         64.04 Warps          
    Achieved Occupancy                                                                   50.03 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  44 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         88 Warps          
    Theoretical Occupancy                                                                68.75 %              
    Waves Per CU                                                                          1.64                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (68.75%) is limited by the number of
required SRF.The difference between calculated theoretical (68.75%) and measured achieved occupancy
(50.03%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}), Context 1, Stream 1, correlationId 1038
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12150 Ns             
    Elapsed Cycles                                                                      663680 Cycles         
    SPP Active Cycles                                                                     8477 Cycles         
    Scheduler(NS) Issue Cycles                                                             276 Cycles         
    ALU Active Cycles                                                                      400 Cycles         
    LSU Active Cycles                                                                     1351 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000415 Inst/Cycle     
    Executed Instructions                                                                  276 Instructions   
    Single-Lane Memory Instructions                                                         16 Instructions   
    Single-Lane ALU Instructions                                                            72 Instructions   
    Single-Lane Control Flow Instructions                                                   48 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            124 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  120 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               4 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                8 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     116 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [512, 1, 1]                
    Launched Warps                                                                           8 Warps          
    Threads                                                                                512 Threads        
    Scalar Register                                                                         16 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                        0.2 %              
    LSU--Global Memory Utilization                                                         0.2 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        80.38 MB/s           
    Global Memory Store Throughput                                                       80.38 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  0.2 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          9.58 Warps          
    Achieved Occupancy                                                                    7.48 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  46 Blocks         
    Block Limit SRF Per CU                                                                  64 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                16 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (7.48%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}), Context 1, Stream 1, correlationId 1054
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             11373 Ns             
    Elapsed Cycles                                                                      665984 Cycles         
    SPP Active Cycles                                                                     8418 Cycles         
    Scheduler(NS) Issue Cycles                                                             276 Cycles         
    ALU Active Cycles                                                                      402 Cycles         
    LSU Active Cycles                                                                     1352 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000445 Inst/Cycle     
    Executed Instructions                                                                  276 Instructions   
    Single-Lane Memory Instructions                                                         16 Instructions   
    Single-Lane ALU Instructions                                                            72 Instructions   
    Single-Lane Control Flow Instructions                                                   48 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            124 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  120 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               4 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                8 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     116 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [512, 1, 1]                
    Launched Warps                                                                           8 Warps          
    Threads                                                                                512 Threads        
    Scalar Register                                                                         16 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                        0.2 %              
    LSU--Global Memory Utilization                                                         0.2 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        85.87 MB/s           
    Global Memory Store Throughput                                                       85.87 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  0.2 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                      0.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         15.78 Warps          
    Achieved Occupancy                                                                   12.33 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  46 Blocks         
    Block Limit SRF Per CU                                                                  64 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                16 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (12.33%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1>), Context 1, Stream 1, correlationId 1067
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            576750 Ns             
    Elapsed Cycles                                                                    36836480 Cycles         
    SPP Active Cycles                                                                 29591193 Cycles         
    Scheduler(NS) Issue Cycles                                                        11809724 Cycles         
    ALU Active Cycles                                                                 14891432 Cycles         
    LSU Active Cycles                                                                 12031985 Cycles         
    L1 Vector Cache Active Cycles                                                      9059304 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Latency Issue --- This kernel exhibits low compute throughput and memory bandwidth utilization relative
to the peak performance of this device.Achieved compute throughput and/or memory bandwidth below 60% of
peak typically indicate latency issues. Look at [Scheduler Statistics] and [Warp State Statistics] for
potential reasons.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.320596 Inst/Cycle     
    Executed Instructions                                                             11814400 Instructions   
    Single-Lane Memory Instructions                                                     454400 Instructions   
    Single-Lane ALU Instructions                                                       3862400 Instructions   
    Single-Lane Control Flow Instructions                                               227200 Instructions   
    Multi-Lane Memory Instructions                                                      908800 Instructions   
     -Multi-Lane Global Memory Instructions                                             908800 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                        6361600 Instructions   
     -Multi-Lane ALU Integer Instructions                                              6361600 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                 6361600 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                   [113600, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                      227200 Warps          
    Threads                                                                           14540800 Threads        
    Scalar Register                                                                         14 Registers/Warp 
    Vector Register                                                                          6 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      32.66 %              
    LSU--Global Memory Utilization                                                       32.66 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                          0.0 B/s            
    Global Memory Store Throughput                                                      375.68 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                24.59 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                        50.0 %              
    L2 Cache Hit Rate                                                                      0.0 %              
    L2 Cache Access to Memory                                                          1817600 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         12.69 Warps          
    Achieved Occupancy                                                                    9.92 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                        110.94                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (9.92%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: ker_input_pad_half2(__half*, __half*, int, int, int, int), Context 1, Stream 1, correlationId 1076
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                           4663.42 Us             
    Elapsed Cycles                                                                   363563264 Cycles         
    SPP Active Cycles                                                                352614916 Cycles         
    Scheduler(NS) Issue Cycles                                                         1762985 Cycles         
    ALU Active Cycles                                                                  2021549 Cycles         
    LSU Active Cycles                                                                349939227 Cycles         
    L1 Vector Cache Active Cycles                                                     86914748 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.005600 Inst/Cycle     
    Executed Instructions                                                              2035296 Instructions   
    Single-Lane Memory Instructions                                                      31968 Instructions   
    Single-Lane ALU Instructions                                                        895104 Instructions   
    Single-Lane Control Flow Instructions                                               106560 Instructions   
    Multi-Lane Memory Instructions                                                       42624 Instructions   
     -Multi-Lane Global Memory Instructions                                              42624 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                         959040 Instructions   
     -Multi-Lane ALU Integer Instructions                                               863136 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                           95904 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                           106560 Instructions   
     +SFU Instructions                                                                   31968 Instructions   
     +BFU Instructions                                                                  820512 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                      [333, 1, 1]                
    Block Size                                                                    [2048, 1, 1]                
    Launched Warps                                                                       10656 Warps          
    Threads                                                                             681984 Threads        
    Scalar Register                                                                         21 Registers/Warp 
    Vector Register                                                                         10 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      96.25 %              
    LSU--Global Memory Utilization                                                       96.25 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         1.09 GB/s           
    Global Memory Store Throughput                                                       17.43 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                23.91 %              
    L1 Vector Cache Load Hit Rate                                                         4.73 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                     2.42 %              
    L2 Cache Access to Memory                                                          1409986 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         88.01 Warps          
    Achieved Occupancy                                                                   68.76 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  12 Blocks         
    Block Limit SRF Per CU                                                                  12 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                 4 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           5.2                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (68.76%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1>), Context 1, Stream 1, correlationId 1088
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12376 Ns             
    Elapsed Cycles                                                                      684416 Cycles         
    SPP Active Cycles                                                                   171431 Cycles         
    Scheduler(NS) Issue Cycles                                                           14761 Cycles         
    ALU Active Cycles                                                                    18672 Cycles         
    LSU Active Cycles                                                                    15116 Cycles         
    L1 Vector Cache Active Cycles                                                        10831 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.14 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.021997 Inst/Cycle     
    Executed Instructions                                                                14976 Instructions   
    Single-Lane Memory Instructions                                                        576 Instructions   
    Single-Lane ALU Instructions                                                          4896 Instructions   
    Single-Lane Control Flow Instructions                                                  288 Instructions   
    Multi-Lane Memory Instructions                                                        1152 Instructions   
     -Multi-Lane Global Memory Instructions                                               1152 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                           8064 Instructions   
     -Multi-Lane ALU Integer Instructions                                                 8064 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                    8064 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                      [144, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                         288 Warps          
    Threads                                                                              18432 Threads        
    Scalar Register                                                                         14 Registers/Warp 
    Vector Register                                                                          6 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       2.21 %              
    LSU--Global Memory Utilization                                                        2.21 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                          0.0 B/s            
    Global Memory Store Throughput                                                       22.19 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 1.58 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                        50.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         16.78 Warps          
    Achieved Occupancy                                                                   13.11 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                          0.14                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (13.11%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void ker_weight_pad_trans<__half>(__half*, __half*, int), Context 1, Stream 1, correlationId 1096
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12773 Ns             
    Elapsed Cycles                                                                      738560 Cycles         
    SPP Active Cycles                                                                   208477 Cycles         
    Scheduler(NS) Issue Cycles                                                           29622 Cycles         
    ALU Active Cycles                                                                    31660 Cycles         
    LSU Active Cycles                                                                    30503 Cycles         
    L1 Vector Cache Active Cycles                                                        19953 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.38 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.040621 Inst/Cycle     
    Executed Instructions                                                                29952 Instructions   
    Single-Lane Memory Instructions                                                       1536 Instructions   
    Single-Lane ALU Instructions                                                          7680 Instructions   
    Single-Lane Control Flow Instructions                                                  768 Instructions   
    Multi-Lane Memory Instructions                                                        1536 Instructions   
     -Multi-Lane Global Memory Instructions                                               1536 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                          18432 Instructions   
     -Multi-Lane ALU Integer Instructions                                                18432 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                              768 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                   17664 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                      [256, 3, 1]                
    Block Size                                                                       [3, 1, 1]                
    Launched Warps                                                                         768 Warps          
    Threads                                                                               2304 Threads        
    Scalar Register                                                                         11 Registers/Warp 
    Vector Register                                                                          5 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Block Size --- Threads are executed in groups of 64 threads called warps. This kernel launch is
configured to execute 3 threads per block. Consequently, some threads in a warp are masked off and those
hardware resources are unused. Try changing the number of threads per block to be a multiple of 64 threads.
Use smaller thread blocks rather than one large thread block per multiprocessor if latency affects
performance. This is particularly beneficial to kernels that frequently call __syncthreads(). 

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       4.13 %              
    LSU--Global Memory Utilization                                                        4.13 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         3.81 GB/s           
    Global Memory Store Throughput                                                       10.75 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  2.7 %              
    L1 Vector Cache Load Hit Rate                                                        61.54 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         21.03 Warps          
    Achieved Occupancy                                                                   16.43 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                               128 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                          0.38                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (16.43%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void cuinfer::impl::kernel::NhwcToN16c64hwn16c64<__half>(unsigned int, unsigned int, unsigned int, unsigned int, __half const*, __half*), Context 1, Stream 1, correlationId 1109
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                           1549.25 Us             
    Elapsed Cycles                                                                    32218112 Cycles         
    SPP Active Cycles                                                                 31777252 Cycles         
    Scheduler(NS) Issue Cycles                                                        14163213 Cycles         
    ALU Active Cycles                                                                 17604962 Cycles         
    LSU Active Cycles                                                                 31551234 Cycles         
    L1 Vector Cache Active Cycles                                                      7859087 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.523144 Inst/Cycle     
    Executed Instructions                                                             16926400 Instructions   
    Single-Lane Memory Instructions                                                     227200 Instructions   
    Single-Lane ALU Instructions                                                       6361600 Instructions   
    Single-Lane Control Flow Instructions                                              2272000 Instructions   
    Multi-Lane Memory Instructions                                                      908800 Instructions   
     -Multi-Lane Global Memory Instructions                                             908800 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                        7156800 Instructions   
     -Multi-Lane ALU Integer Instructions                                              7156800 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                           227200 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                 6929600 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                    [14200, 1, 2]                
    Block Size                                                                     [16, 16, 1]                
    Launched Warps                                                                      113600 Warps          
    Threads                                                                            7270400 Threads        
    Scalar Register                                                                         34 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      97.93 %              
    LSU--Global Memory Utilization                                                       97.93 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        69.93 GB/s           
    Global Memory Store Throughput                                                       69.93 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                24.39 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                     37.5 %              
    L2 Cache Access to Memory                                                          3635080 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                        124.35 Warps          
    Achieved Occupancy                                                                   97.15 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  93 Blocks         
    Block Limit SRF Per CU                                                                  60 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                32 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                         55.47                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. 

  Kernel: void cuinfer::impl::kernel::implConvolution2DTcuKernelFp16Template_N16_3x3_1x1_DB<256u, 256u, 32u, 64u, 64u, 3, 3, true, false, false, 1, 1, 0, 0, __half, __half, float>(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, __half*, float const*, float, float, float, float, __half const*, unsigned int, unsigned int, unsigned int, unsigned int, bool), Context 1, Stream 1, correlationId 1114
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            1831.5 Us             
    Elapsed Cycles                                                                   116036480 Cycles         
    SPP Active Cycles                                                                114440092 Cycles         
    Scheduler(NS) Issue Cycles                                                        52616120 Cycles         
    ALU Active Cycles                                                                 99631554 Cycles         
    LSU Active Cycles                                                                 78736466 Cycles         
    L1 Vector Cache Active Cycles                                                     23018653 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Compute Workload Analysis section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.552142 Inst/Cycle     
    Executed Instructions                                                             64483208 Instructions   
    Single-Lane Memory Instructions                                                     193312 Instructions   
    Single-Lane ALU Instructions                                                      21926872 Instructions   
    Single-Lane Control Flow Instructions                                              1325532 Instructions   
    Multi-Lane Memory Instructions                                                    11985452 Instructions   
     -Multi-Lane Global Memory Instructions                                             994176 Instructions   
     -Multi-Lane Shared Memory Instructions                                           10494224 Instructions   
     -Multi-Lane SME Memory Instructions                                                497052 Instructions   
    Multi-Lane ALU Instructions                                                       29052040 Instructions   
     -Multi-Lane ALU Integer Instructions                                             15796360 Instructions   
     -Multi-Lane ALU Half Float Instructions                                           1767424 Instructions   
     -Multi-Lane ALU Single Float Instructions                                        11488256 Instructions   
     +Multi-Lane Matrix Instructions                                                   7953408 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                          1795040 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                19303592 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                     [1, 1726, 1]                
    Block Size                                                                    [1024, 1, 1]                
    Launched Warps                                                                       27616 Warps          
    Threads                                                                            1767424 Threads        
    Scalar Register                                                                         61 Registers/Warp 
    Vector Register                                                                        103 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                 65536 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      67.85 %              
    LSU--Global Memory Utilization                                                       11.26 %              
    LSU--Shared Memory Utilization                                                       61.05 %              
    Global Memory Load Throughput                                                         3.59 GB/s           
    Global Memory Store Throughput                                                      115.02 GB/s           
    Shared Memory Load Throughput                                                      1035.35 GB/s           
    Shared Memory Store Throughput                                                        19.2 MB/s           
    Shared Store Bank Conflict                                                        10604544 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                19.84 %              
    L1 Vector Cache Load Hit Rate                                                        87.47 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    65.34 %              
    L2 Cache Access to Memory                                                          5324972 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         31.68 Warps          
    Achieved Occupancy                                                                   24.75 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                   2 Blocks         
    Block Limit SRF Per CU                                                                   8 Blocks         
    Block Limit Shared Mem Per CU                                                            2 Blocks         
    Block Limit Warps Per CU                                                                 8 Blocks         
    Theoretical Active Warps per CU                                                         32 Warps          
    Theoretical Occupancy                                                                 25.0 %              
    Waves Per CU                                                                         53.94                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (25.0%) is limited by the number of required
VRF.This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory.

  Kernel: void cuinfer::impl::kernel::NhwcToN16c64hwn16c64<__half>(unsigned int, unsigned int, unsigned int, unsigned int, __half const*, __half*), Context 1, Stream 1, correlationId 1124
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                           1954.58 Us             
    Elapsed Cycles                                                                   124124672 Cycles         
    SPP Active Cycles                                                                123582307 Cycles         
    Scheduler(NS) Issue Cycles                                                        29543101 Cycles         
    ALU Active Cycles                                                                 38437294 Cycles         
    LSU Active Cycles                                                                123331333 Cycles         
    L1 Vector Cache Active Cycles                                                     29501409 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.265111 Inst/Cycle     
    Executed Instructions                                                             32915968 Instructions   
    Single-Lane Memory Instructions                                                     441856 Instructions   
    Single-Lane ALU Instructions                                                      12371456 Instructions   
    Single-Lane Control Flow Instructions                                              4418176 Instructions   
    Multi-Lane Memory Instructions                                                     1767168 Instructions   
     -Multi-Lane Global Memory Instructions                                            1767168 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                       13917312 Instructions   
     -Multi-Lane ALU Integer Instructions                                             13917312 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                           441856 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                13475456 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                     [3452, 8, 2]                
    Block Size                                                                     [16, 16, 1]                
    Launched Warps                                                                      220928 Warps          
    Threads                                                                           14139392 Threads        
    Scalar Register                                                                         34 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      99.36 %              
    LSU--Global Memory Utilization                                                       99.36 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       107.78 GB/s           
    Global Memory Store Throughput                                                      107.78 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                23.77 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                      0.0 %              
    L2 Cache Access to Memory                                                         17671704 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                        125.46 Warps          
    Achieved Occupancy                                                                   98.02 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  93 Blocks         
    Block Limit SRF Per CU                                                                  60 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                32 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                        107.88                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. 

  Kernel: void cuinfer::impl::kernel::implConvolution2DTcuKernelFp16Template_N16_3x3_1x1_DB<256u, 256u, 32u, 64u, 64u, 3, 3, true, false, false, 1, 1, 0, 0, __half, __half, float>(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, __half*, float const*, float, float, float, float, __half const*, unsigned int, unsigned int, unsigned int, unsigned int, bool), Context 1, Stream 1, correlationId 1128
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            2391.9 Us             
    Elapsed Cycles                                                                   155389952 Cycles         
    SPP Active Cycles                                                                150222632 Cycles         
    Scheduler(NS) Issue Cycles                                                        52170509 Cycles         
    ALU Active Cycles                                                                133586995 Cycles         
    LSU Active Cycles                                                                118255102 Cycles         
    L1 Vector Cache Active Cycles                                                     30781926 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Compute Workload Analysis section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.441242 Inst/Cycle     
    Executed Instructions                                                             68705824 Instructions   
    Single-Lane Memory Instructions                                                      46816 Instructions   
    Single-Lane ALU Instructions                                                      30169568 Instructions   
    Single-Lane Control Flow Instructions                                              2427744 Instructions   
    Multi-Lane Memory Instructions                                                    15542912 Instructions   
     -Multi-Lane Global Memory Instructions                                             240768 Instructions   
     -Multi-Lane Shared Memory Instructions                                           14339072 Instructions   
     -Multi-Lane SME Memory Instructions                                                963072 Instructions   
    Multi-Lane ALU Instructions                                                       20518784 Instructions   
     -Multi-Lane ALU Integer Instructions                                              3825536 Instructions   
     -Multi-Lane ALU Half Float Instructions                                            428032 Instructions   
     -Multi-Lane ALU Single Float Instructions                                        16265216 Instructions   
     +Multi-Lane Matrix Instructions                                                  15409152 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                           434720 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                 4674912 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                      [1, 418, 1]                
    Block Size                                                                    [1024, 1, 1]                
    Launched Warps                                                                        6688 Warps          
    Threads                                                                             428032 Threads        
    Scalar Register                                                                         61 Registers/Warp 
    Vector Register                                                                        103 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                 65536 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       76.1 %              
    LSU--Global Memory Utilization                                                        2.05 %              
    LSU--Shared Memory Utilization                                                       74.82 %              
    Global Memory Load Throughput                                                       682.64 MB/s           
    Global Memory Store Throughput                                                       21.33 GB/s           
    Shared Memory Load Throughput                                                      1535.95 GB/s           
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                         2568192 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                19.81 %              
    L1 Vector Cache Load Hit Rate                                                         87.5 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    87.55 %              
    L2 Cache Access to Memory                                                          5498700 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         31.86 Warps          
    Achieved Occupancy                                                                   24.89 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                   2 Blocks         
    Block Limit SRF Per CU                                                                   8 Blocks         
    Block Limit Shared Mem Per CU                                                            2 Blocks         
    Block Limit Warps Per CU                                                                 8 Blocks         
    Theoretical Active Warps per CU                                                         32 Warps          
    Theoretical Occupancy                                                                 25.0 %              
    Waves Per CU                                                                         13.06                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (25.0%) is limited by the number of required
VRF.This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory.

  Kernel: void at::native::modern::elementwise_kernel<at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1>), Context 1, Stream 1, correlationId 1139
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            209150 Ns             
    Elapsed Cycles                                                                    13328768 Cycles         
    SPP Active Cycles                                                                 10225631 Cycles         
    Scheduler(NS) Issue Cycles                                                         4168175 Cycles         
    ALU Active Cycles                                                                  5253831 Cycles         
    LSU Active Cycles                                                                  4245918 Cycles         
    L1 Vector Cache Active Cycles                                                      3177373 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Latency Issue --- This kernel exhibits low compute throughput and memory bandwidth utilization relative
to the peak performance of this device.Achieved compute throughput and/or memory bandwidth below 60% of
peak typically indicate latency issues. Look at [Scheduler Statistics] and [Warp State Statistics] for
potential reasons.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.312784 Inst/Cycle     
    Executed Instructions                                                              4173312 Instructions   
    Single-Lane Memory Instructions                                                     160512 Instructions   
    Single-Lane ALU Instructions                                                       1364352 Instructions   
    Single-Lane Control Flow Instructions                                                80256 Instructions   
    Multi-Lane Memory Instructions                                                      321024 Instructions   
     -Multi-Lane Global Memory Instructions                                             321024 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                        2247168 Instructions   
     -Multi-Lane ALU Integer Instructions                                              2247168 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                 2247168 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                    [40128, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                       80256 Warps          
    Threads                                                                            5136384 Threads        
    Scalar Register                                                                         14 Registers/Warp 
    Vector Register                                                                          6 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      31.86 %              
    LSU--Global Memory Utilization                                                       31.86 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                          0.0 B/s            
    Global Memory Store Throughput                                                      365.95 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                23.84 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                        50.0 %              
    L2 Cache Hit Rate                                                                      0.0 %              
    L2 Cache Access to Memory                                                           642048 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         13.45 Warps          
    Achieved Occupancy                                                                   10.51 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                         39.19                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (10.51%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void ker_output_data<__half>(__half*, __half*), Context 1, Stream 1, correlationId 1147
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            337500 Ns             
    Elapsed Cycles                                                                    21494144 Cycles         
    SPP Active Cycles                                                                 20973243 Cycles         
    Scheduler(NS) Issue Cycles                                                         7212484 Cycles         
    ALU Active Cycles                                                                  8033570 Cycles         
    LSU Active Cycles                                                                 18688199 Cycles         
    L1 Vector Cache Active Cycles                                                      5200020 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.373322 Inst/Cycle     
    Executed Instructions                                                              8027200 Instructions   
    Single-Lane Memory Instructions                                                     321088 Instructions   
    Single-Lane ALU Instructions                                                       2247616 Instructions   
    Single-Lane Control Flow Instructions                                               321088 Instructions   
    Multi-Lane Memory Instructions                                                      642176 Instructions   
     -Multi-Lane Global Memory Instructions                                             642176 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                        4495232 Instructions   
     -Multi-Lane ALU Integer Instructions                                              4495232 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                 4495232 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                     [5017, 1, 1]                
    Block Size                                                                    [4096, 1, 1]                
    Launched Warps                                                                      321088 Warps          
    Threads                                                                           20549632 Threads        
    Scalar Register                                                                         10 Registers/Warp 
    Vector Register                                                                          5 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      86.95 %              
    LSU--Global Memory Utilization                                                       86.95 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       113.41 GB/s           
    Global Memory Store Throughput                                                      226.82 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                24.19 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                        50.0 %              
    L2 Cache Hit Rate                                                                      0.0 %              
    L2 Cache Access to Memory                                                          1284552 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         52.18 Warps          
    Achieved Occupancy                                                                   40.76 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  12 Blocks         
    Block Limit SRF Per CU                                                                  12 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                 2 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                        156.78                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (40.76%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}), Context 1, Stream 1, correlationId 1163
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12123 Ns             
    Elapsed Cycles                                                                      661376 Cycles         
    SPP Active Cycles                                                                     5146 Cycles         
    Scheduler(NS) Issue Cycles                                                             190 Cycles         
    ALU Active Cycles                                                                      254 Cycles         
    LSU Active Cycles                                                                      682 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000286 Inst/Cycle     
    Executed Instructions                                                                  190 Instructions   
    Single-Lane Memory Instructions                                                         12 Instructions   
    Single-Lane ALU Instructions                                                            60 Instructions   
    Single-Lane Control Flow Instructions                                                   32 Instructions   
    Multi-Lane Memory Instructions                                                           8 Instructions   
     -Multi-Lane Global Memory Instructions                                                  8 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             78 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   76 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               2 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                4 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      74 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [512, 1, 1]                
    Launched Warps                                                                           8 Warps          
    Threads                                                                                512 Threads        
    Scalar Register                                                                         16 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                        0.1 %              
    LSU--Global Memory Utilization                                                         0.1 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        45.31 MB/s           
    Global Memory Store Throughput                                                       35.24 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  0.2 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          13.0 Warps          
    Achieved Occupancy                                                                   10.16 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  46 Blocks         
    Block Limit SRF Per CU                                                                  64 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                16 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (10.16%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 1179
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             10903 Ns             
    Elapsed Cycles                                                                      571520 Cycles         
    SPP Active Cycles                                                                     2576 Cycles         
    Scheduler(NS) Issue Cycles                                                             170 Cycles         
    ALU Active Cycles                                                                      199 Cycles         
    LSU Active Cycles                                                                      191 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000300 Inst/Cycle     
    Executed Instructions                                                                  170 Instructions   
    Single-Lane Memory Instructions                                                          4 Instructions   
    Single-Lane ALU Instructions                                                            66 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             82 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   74 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               8 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      82 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         18 Registers/Warp 
    Vector Register                                                                          8 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        39.19 MB/s           
    Global Memory Store Throughput                                                       39.19 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.97 Warps          
    Achieved Occupancy                                                                    6.23 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.23%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 1189
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                              9726 Ns             
    Elapsed Cycles                                                                      561152 Cycles         
    SPP Active Cycles                                                                     1900 Cycles         
    Scheduler(NS) Issue Cycles                                                             180 Cycles         
    ALU Active Cycles                                                                      209 Cycles         
    LSU Active Cycles                                                                      192 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000322 Inst/Cycle     
    Executed Instructions                                                                  180 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            66 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             90 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   82 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               8 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      90 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         20 Registers/Warp 
    Vector Register                                                                          8 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        43.93 MB/s           
    Global Memory Store Throughput                                                        50.2 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           4.0 Warps          
    Achieved Occupancy                                                                    3.13 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (3.13%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3>), Context 1, Stream 1, correlationId 1199
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             10026 Ns             
    Elapsed Cycles                                                                      565760 Cycles         
    SPP Active Cycles                                                                     2084 Cycles         
    Scheduler(NS) Issue Cycles                                                             206 Cycles         
    ALU Active Cycles                                                                      227 Cycles         
    LSU Active Cycles                                                                      189 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000363 Inst/Cycle     
    Executed Instructions                                                                  206 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            66 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                          24 Instructions   
     -Multi-Lane Global Memory Instructions                                                 24 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            108 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  100 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               8 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     108 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         18 Registers/Warp 
    Vector Register                                                                         14 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        85.23 MB/s           
    Global Memory Store Throughput                                                        48.7 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.98 Warps          
    Achieved Occupancy                                                                    6.23 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.23%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3>), Context 1, Stream 1, correlationId 1209
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             10546 Ns             
    Elapsed Cycles                                                                      562304 Cycles         
    SPP Active Cycles                                                                     2229 Cycles         
    Scheduler(NS) Issue Cycles                                                             204 Cycles         
    ALU Active Cycles                                                                      248 Cycles         
    LSU Active Cycles                                                                      210 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000334 Inst/Cycle     
    Executed Instructions                                                                  204 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            78 Instructions   
    Single-Lane Control Flow Instructions                                                    4 Instructions   
    Multi-Lane Memory Instructions                                                          20 Instructions   
     -Multi-Lane Global Memory Instructions                                                 20 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             96 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   96 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      96 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         22 Registers/Warp 
    Vector Register                                                                         14 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.04 %              
    LSU--Global Memory Utilization                                                        0.04 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        23.15 MB/s           
    Global Memory Store Throughput                                                        46.3 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.97 Warps          
    Achieved Occupancy                                                                    6.23 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.23%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 1219
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                              9823 Ns             
    Elapsed Cycles                                                                      562304 Cycles         
    SPP Active Cycles                                                                     1921 Cycles         
    Scheduler(NS) Issue Cycles                                                             180 Cycles         
    ALU Active Cycles                                                                      208 Cycles         
    LSU Active Cycles                                                                      189 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000322 Inst/Cycle     
    Executed Instructions                                                                  180 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            66 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             90 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   82 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               8 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      90 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         20 Registers/Warp 
    Vector Register                                                                          8 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        43.49 MB/s           
    Global Memory Store Throughput                                                       49.71 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.97 Warps          
    Achieved Occupancy                                                                    6.22 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.22%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3>), Context 1, Stream 1, correlationId 1228
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             10823 Ns             
    Elapsed Cycles                                                                      580736 Cycles         
    SPP Active Cycles                                                                     2213 Cycles         
    Scheduler(NS) Issue Cycles                                                             204 Cycles         
    ALU Active Cycles                                                                      253 Cycles         
    LSU Active Cycles                                                                      193 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000353 Inst/Cycle     
    Executed Instructions                                                                  204 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            78 Instructions   
    Single-Lane Control Flow Instructions                                                    4 Instructions   
    Multi-Lane Memory Instructions                                                          20 Instructions   
     -Multi-Lane Global Memory Instructions                                                 20 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             96 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   96 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      96 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         22 Registers/Warp 
    Vector Register                                                                         14 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        22.56 MB/s           
    Global Memory Store Throughput                                                       45.12 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.23 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.92 Warps          
    Achieved Occupancy                                                                    6.19 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.19%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::cuda::detail::cub::DeviceReduceSingleTileKernel<at::cuda::detail::cub::DeviceReducePolicy<bool, int, int, at::cuda::detail::cub::Sum>::Policy600, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int>(at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int), Context 1, Stream 1, correlationId 1256
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             11850 Ns             
    Elapsed Cycles                                                                      640640 Cycles         
    SPP Active Cycles                                                                     6333 Cycles         
    Scheduler(NS) Issue Cycles                                                             521 Cycles         
    ALU Active Cycles                                                                      632 Cycles         
    LSU Active Cycles                                                                      432 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000812 Inst/Cycle     
    Executed Instructions                                                                  521 Instructions   
    Single-Lane Memory Instructions                                                         16 Instructions   
    Single-Lane ALU Instructions                                                           151 Instructions   
    Single-Lane Control Flow Instructions                                                   39 Instructions   
    Multi-Lane Memory Instructions                                                          37 Instructions   
     -Multi-Lane Global Memory Instructions                                                  8 Instructions   
     -Multi-Lane Shared Memory Instructions                                                 29 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            278 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  278 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     278 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [256, 1, 1]                
    Launched Warps                                                                           4 Warps          
    Threads                                                                                256 Threads        
    Scalar Register                                                                         18 Registers/Warp 
    Vector Register                                                                         48 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                    24 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.07 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                        0.04 %              
    Global Memory Load Throughput                                                         10.3 MB/s           
    Global Memory Store Throughput                                                        5.15 MB/s           
    Shared Memory Load Throughput                                                         5.15 MB/s           
    Shared Memory Store Throughput                                                        20.6 MB/s           
    Shared Store Bank Conflict                                                              72 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.21 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         15.19 Warps          
    Achieved Occupancy                                                                   11.87 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  21 Blocks         
    Block Limit SRF Per CU                                                                 113 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                32 Blocks         
    Theoretical Active Warps per CU                                                         84 Warps          
    Theoretical Occupancy                                                                65.62 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (65.62%) is limited by the number of
required VRF.The difference between calculated theoretical (65.62%) and measured achieved occupancy
(11.87%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::cuda::detail::cub::DeviceCompactInitKernel<at::cuda::detail::cub::ScanTileState<int, true>, int*>(at::cuda::detail::cub::ScanTileState<int, true>, int, int*), Context 1, Stream 1, correlationId 1274
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                              9723 Ns             
    Elapsed Cycles                                                                      561152 Cycles         
    SPP Active Cycles                                                                     1897 Cycles         
    Scheduler(NS) Issue Cycles                                                              90 Cycles         
    ALU Active Cycles                                                                      101 Cycles         
    LSU Active Cycles                                                                       60 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000161 Inst/Cycle     
    Executed Instructions                                                                   90 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            36 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                           6 Instructions   
     -Multi-Lane Global Memory Instructions                                                  6 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             40 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   40 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      40 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         16 Registers/Warp 
    Vector Register                                                                          4 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                          0.0 B/s            
    Global Memory Store Throughput                                                       37.66 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.97 Warps          
    Achieved Occupancy                                                                    6.23 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.23%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::cuda::detail::cub::DeviceSelectSweepKernel<at::cuda::detail::cub::DispatchSelectIf<at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>::PtxSelectIfPolicyT, at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>(at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, int), Context 1, Stream 1, correlationId 1281
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12953 Ns             
    Elapsed Cycles                                                                      718976 Cycles         
    SPP Active Cycles                                                                     4154 Cycles         
    Scheduler(NS) Issue Cycles                                                             454 Cycles         
    ALU Active Cycles                                                                      636 Cycles         
    LSU Active Cycles                                                                      500 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000636 Inst/Cycle     
    Executed Instructions                                                                  454 Instructions   
    Single-Lane Memory Instructions                                                         14 Instructions   
    Single-Lane ALU Instructions                                                           124 Instructions   
    Single-Lane Control Flow Instructions                                                   20 Instructions   
    Multi-Lane Memory Instructions                                                          54 Instructions   
     -Multi-Lane Global Memory Instructions                                                 14 Instructions   
     -Multi-Lane Shared Memory Instructions                                                 40 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            242 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  242 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     242 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         54 Registers/Warp 
    Vector Register                                                                         37 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                  3072 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.07 %              
    LSU--Global Memory Utilization                                                        0.02 %              
    LSU--Shared Memory Utilization                                                        0.04 %              
    Global Memory Load Throughput                                                         9.42 MB/s           
    Global Memory Store Throughput                                                      202.62 MB/s           
    Shared Memory Load Throughput                                                       697.38 MB/s           
    Shared Memory Store Throughput                                                      574.87 MB/s           
    Shared Store Bank Conflict                                                             102 Conflicts      
    Shared Load Bank Conflict                                                               78 Conflicts      
    L1VK Unit Utilization                                                                 0.19 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.99 Warps          
    Achieved Occupancy                                                                    6.24 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  55 Blocks         
    Block Limit SRF Per CU                                                                  75 Blocks         
    Block Limit Shared Mem Per CU                                                           42 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         84 Warps          
    Theoretical Occupancy                                                                65.62 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (65.62%) is limited by the required amount
of shared memory.The difference between calculated theoretical (65.62%) and measured achieved occupancy
(6.24%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::index_elementwise_kernel<128, 4, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}), Context 1, Stream 1, correlationId 1294
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             16326 Ns             
    Elapsed Cycles                                                                      921728 Cycles         
    SPP Active Cycles                                                                     7739 Cycles         
    Scheduler(NS) Issue Cycles                                                             158 Cycles         
    ALU Active Cycles                                                                      179 Cycles         
    LSU Active Cycles                                                                      127 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000172 Inst/Cycle     
    Executed Instructions                                                                  158 Instructions   
    Single-Lane Memory Instructions                                                         12 Instructions   
    Single-Lane ALU Instructions                                                            63 Instructions   
    Single-Lane Control Flow Instructions                                                   17 Instructions   
    Multi-Lane Memory Instructions                                                           3 Instructions   
     -Multi-Lane Global Memory Instructions                                                  3 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             63 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   63 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                8 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      55 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         50 Registers/Warp 
    Vector Register                                                                         13 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        78.51 MB/s           
    Global Memory Store Throughput                                                       14.95 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.15 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          5.22 Warps          
    Achieved Occupancy                                                                    4.08 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  81 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (4.08%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 1309
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             10526 Ns             
    Elapsed Cycles                                                                      562304 Cycles         
    SPP Active Cycles                                                                     4028 Cycles         
    Scheduler(NS) Issue Cycles                                                             170 Cycles         
    ALU Active Cycles                                                                      197 Cycles         
    LSU Active Cycles                                                                      152 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000304 Inst/Cycle     
    Executed Instructions                                                                  170 Instructions   
    Single-Lane Memory Instructions                                                          4 Instructions   
    Single-Lane ALU Instructions                                                            66 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             82 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   74 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               8 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      82 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         18 Registers/Warp 
    Vector Register                                                                          8 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        23.19 MB/s           
    Global Memory Store Throughput                                                       23.19 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.97 Warps          
    Achieved Occupancy                                                                    6.22 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.22%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4>), Context 1, Stream 1, correlationId 1324
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             17276 Ns             
    Elapsed Cycles                                                                      989696 Cycles         
    SPP Active Cycles                                                                     9068 Cycles         
    Scheduler(NS) Issue Cycles                                                             319 Cycles         
    ALU Active Cycles                                                                      338 Cycles         
    LSU Active Cycles                                                                      125 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000318 Inst/Cycle     
    Executed Instructions                                                                  319 Instructions   
    Single-Lane Memory Instructions                                                         22 Instructions   
    Single-Lane ALU Instructions                                                           130 Instructions   
    Single-Lane Control Flow Instructions                                                   31 Instructions   
    Multi-Lane Memory Instructions                                                           8 Instructions   
     -Multi-Lane Global Memory Instructions                                                  3 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  5 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            128 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   98 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                              30 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                4 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     124 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                      [32, 1, 1]                
    Launched Warps                                                                           1 Warps          
    Threads                                                                                 32 Threads        
    Scalar Register                                                                        102 Registers/Warp 
    Vector Register                                                                         22 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     4 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Block Size --- Threads are executed in groups of 64 threads called warps. This kernel launch is
configured to execute 32 threads per block. Consequently, some threads in a warp are masked off and those
hardware resources are unused. Try changing the number of threads per block to be a multiple of 64 threads.
Use smaller thread blocks rather than one large thread block per multiprocessor if latency affects
performance. This is particularly beneficial to kernels that frequently call __syncthreads(). 
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        14.13 MB/s           
    Global Memory Store Throughput                                                        3.53 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                              15 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.14 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           4.0 Warps          
    Achieved Occupancy                                                                    3.12 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  80 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                               128 Blocks         
    Theoretical Active Warps per CU                                                         80 Warps          
    Theoretical Occupancy                                                                 62.5 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (62.5%) is limited by the number of required
SRF.The difference between calculated theoretical (62.5%) and measured achieved occupancy (3.12%) can be
the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4>), Context 1, Stream 1, correlationId 1340
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             17926 Ns             
    Elapsed Cycles                                                                     1005824 Cycles         
    SPP Active Cycles                                                                     8753 Cycles         
    Scheduler(NS) Issue Cycles                                                             319 Cycles         
    ALU Active Cycles                                                                      322 Cycles         
    LSU Active Cycles                                                                      125 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000308 Inst/Cycle     
    Executed Instructions                                                                  319 Instructions   
    Single-Lane Memory Instructions                                                         22 Instructions   
    Single-Lane ALU Instructions                                                           130 Instructions   
    Single-Lane Control Flow Instructions                                                   31 Instructions   
    Multi-Lane Memory Instructions                                                           8 Instructions   
     -Multi-Lane Global Memory Instructions                                                  3 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  5 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            128 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   98 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                              30 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                4 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     124 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                      [32, 1, 1]                
    Launched Warps                                                                           1 Warps          
    Threads                                                                                 32 Threads        
    Scalar Register                                                                        102 Registers/Warp 
    Vector Register                                                                         22 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     4 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Block Size --- Threads are executed in groups of 64 threads called warps. This kernel launch is
configured to execute 32 threads per block. Consequently, some threads in a warp are masked off and those
hardware resources are unused. Try changing the number of threads per block to be a multiple of 64 threads.
Use smaller thread blocks rather than one large thread block per multiprocessor if latency affects
performance. This is particularly beneficial to kernels that frequently call __syncthreads(). 
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        13.62 MB/s           
    Global Memory Store Throughput                                                         3.4 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                              15 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.13 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           4.0 Warps          
    Achieved Occupancy                                                                    3.12 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  80 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                               128 Blocks         
    Theoretical Active Warps per CU                                                         80 Warps          
    Theoretical Occupancy                                                                 62.5 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (62.5%) is limited by the number of required
SRF.The difference between calculated theoretical (62.5%) and measured achieved occupancy (3.12%) can be
the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 1349
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12950 Ns             
    Elapsed Cycles                                                                      723584 Cycles         
    SPP Active Cycles                                                                     9439 Cycles         
    Scheduler(NS) Issue Cycles                                                             268 Cycles         
    ALU Active Cycles                                                                      186 Cycles         
    LSU Active Cycles                                                                       70 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000374 Inst/Cycle     
    Executed Instructions                                                                  268 Instructions   
    Single-Lane Memory Instructions                                                         22 Instructions   
    Single-Lane ALU Instructions                                                           113 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           2 Instructions   
     -Multi-Lane Global Memory Instructions                                                  2 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            120 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  119 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               1 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       1 Instructions   
     +BFU Instructions                                                                     119 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         92 Registers/Warp 
    Vector Register                                                                          9 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         4.71 MB/s           
    Global Memory Store Throughput                                                        4.71 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.19 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           8.0 Warps          
    Achieved Occupancy                                                                    6.25 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  44 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         88 Warps          
    Theoretical Occupancy                                                                68.75 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (68.75%) is limited by the number of
required SRF.The difference between calculated theoretical (68.75%) and measured achieved occupancy (6.25%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 1359
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             13776 Ns             
    Elapsed Cycles                                                                      768512 Cycles         
    SPP Active Cycles                                                                    10627 Cycles         
    Scheduler(NS) Issue Cycles                                                             319 Cycles         
    ALU Active Cycles                                                                      233 Cycles         
    LSU Active Cycles                                                                       72 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000415 Inst/Cycle     
    Executed Instructions                                                                  319 Instructions   
    Single-Lane Memory Instructions                                                         26 Instructions   
    Single-Lane ALU Instructions                                                           121 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           3 Instructions   
     -Multi-Lane Global Memory Instructions                                                  3 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            158 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  157 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               1 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     158 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         94 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         8.86 MB/s           
    Global Memory Store Throughput                                                        4.43 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.17 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           8.0 Warps          
    Achieved Occupancy                                                                    6.25 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  43 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         86 Warps          
    Theoretical Occupancy                                                                67.19 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (67.19%) is limited by the number of
required SRF.The difference between calculated theoretical (67.19%) and measured achieved occupancy (6.25%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 1373
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             14376 Ns             
    Elapsed Cycles                                                                      818048 Cycles         
    SPP Active Cycles                                                                     5570 Cycles         
    Scheduler(NS) Issue Cycles                                                             324 Cycles         
    ALU Active Cycles                                                                      241 Cycles         
    LSU Active Cycles                                                                       81 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000416 Inst/Cycle     
    Executed Instructions                                                                  324 Instructions   
    Single-Lane Memory Instructions                                                         26 Instructions   
    Single-Lane ALU Instructions                                                           121 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           3 Instructions   
     -Multi-Lane Global Memory Instructions                                                  3 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            163 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  157 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               6 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       1 Instructions   
     +BFU Instructions                                                                     162 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         94 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         8.49 MB/s           
    Global Memory Store Throughput                                                        4.25 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.16 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.99 Warps          
    Achieved Occupancy                                                                    6.24 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  43 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         86 Warps          
    Theoretical Occupancy                                                                67.19 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (67.19%) is limited by the number of
required SRF.The difference between calculated theoretical (67.19%) and measured achieved occupancy (6.24%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 1382
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12100 Ns             
    Elapsed Cycles                                                                      715520 Cycles         
    SPP Active Cycles                                                                     4006 Cycles         
    Scheduler(NS) Issue Cycles                                                             272 Cycles         
    ALU Active Cycles                                                                      189 Cycles         
    LSU Active Cycles                                                                       72 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000383 Inst/Cycle     
    Executed Instructions                                                                  272 Instructions   
    Single-Lane Memory Instructions                                                         24 Instructions   
    Single-Lane ALU Instructions                                                           113 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           2 Instructions   
     -Multi-Lane Global Memory Instructions                                                  2 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            122 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  121 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               1 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     122 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         94 Registers/Warp 
    Vector Register                                                                          9 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         5.04 MB/s           
    Global Memory Store Throughput                                                        5.04 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.19 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.99 Warps          
    Achieved Occupancy                                                                    6.24 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  43 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         86 Warps          
    Theoretical Occupancy                                                                67.19 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (67.19%) is limited by the number of
required SRF.The difference between calculated theoretical (67.19%) and measured achieved occupancy (6.24%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 1397
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             11950 Ns             
    Elapsed Cycles                                                                      665984 Cycles         
    SPP Active Cycles                                                                     4006 Cycles         
    Scheduler(NS) Issue Cycles                                                             272 Cycles         
    ALU Active Cycles                                                                      188 Cycles         
    LSU Active Cycles                                                                       70 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000398 Inst/Cycle     
    Executed Instructions                                                                  272 Instructions   
    Single-Lane Memory Instructions                                                         24 Instructions   
    Single-Lane ALU Instructions                                                           113 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           2 Instructions   
     -Multi-Lane Global Memory Instructions                                                  2 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            122 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  121 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               1 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     122 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         94 Registers/Warp 
    Vector Register                                                                          9 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         5.11 MB/s           
    Global Memory Store Throughput                                                        5.11 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  0.2 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.68 Warps          
    Achieved Occupancy                                                                     6.0 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  43 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         86 Warps          
    Theoretical Occupancy                                                                67.19 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (67.19%) is limited by the number of
required SRF.The difference between calculated theoretical (67.19%) and measured achieved occupancy (6.0%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 1411
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             11926 Ns             
    Elapsed Cycles                                                                      687872 Cycles         
    SPP Active Cycles                                                                     4000 Cycles         
    Scheduler(NS) Issue Cycles                                                             272 Cycles         
    ALU Active Cycles                                                                      189 Cycles         
    LSU Active Cycles                                                                       72 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000383 Inst/Cycle     
    Executed Instructions                                                                  272 Instructions   
    Single-Lane Memory Instructions                                                         24 Instructions   
    Single-Lane ALU Instructions                                                           113 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           2 Instructions   
     -Multi-Lane Global Memory Instructions                                                  2 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            122 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  121 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               1 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     122 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         94 Registers/Warp 
    Vector Register                                                                          9 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         5.12 MB/s           
    Global Memory Store Throughput                                                        5.12 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  0.2 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           4.0 Warps          
    Achieved Occupancy                                                                    3.13 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  43 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         86 Warps          
    Theoretical Occupancy                                                                67.19 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (67.19%) is limited by the number of
required SRF.The difference between calculated theoretical (67.19%) and measured achieved occupancy (3.13%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}), Context 1, Stream 1, correlationId 1810
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12176 Ns             
    Elapsed Cycles                                                                      663680 Cycles         
    SPP Active Cycles                                                                     6874 Cycles         
    Scheduler(NS) Issue Cycles                                                             190 Cycles         
    ALU Active Cycles                                                                      253 Cycles         
    LSU Active Cycles                                                                      677 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000285 Inst/Cycle     
    Executed Instructions                                                                  190 Instructions   
    Single-Lane Memory Instructions                                                         12 Instructions   
    Single-Lane ALU Instructions                                                            60 Instructions   
    Single-Lane Control Flow Instructions                                                   32 Instructions   
    Multi-Lane Memory Instructions                                                           8 Instructions   
     -Multi-Lane Global Memory Instructions                                                  8 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             78 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   76 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               2 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                4 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      74 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [512, 1, 1]                
    Launched Warps                                                                           8 Warps          
    Threads                                                                                512 Threads        
    Scalar Register                                                                         16 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                        0.1 %              
    LSU--Global Memory Utilization                                                         0.1 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        45.11 MB/s           
    Global Memory Store Throughput                                                       35.09 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  0.2 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         13.03 Warps          
    Achieved Occupancy                                                                   10.18 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  46 Blocks         
    Block Limit SRF Per CU                                                                  64 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                16 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (10.18%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 1826
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                              9803 Ns             
    Elapsed Cycles                                                                      564608 Cycles         
    SPP Active Cycles                                                                     4133 Cycles         
    Scheduler(NS) Issue Cycles                                                             170 Cycles         
    ALU Active Cycles                                                                      199 Cycles         
    LSU Active Cycles                                                                      187 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000304 Inst/Cycle     
    Executed Instructions                                                                  170 Instructions   
    Single-Lane Memory Instructions                                                          4 Instructions   
    Single-Lane ALU Instructions                                                            66 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             82 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   74 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               8 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      82 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         18 Registers/Warp 
    Vector Register                                                                          8 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        43.58 MB/s           
    Global Memory Store Throughput                                                       43.58 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.99 Warps          
    Achieved Occupancy                                                                    6.24 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.24%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 1835
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                              9850 Ns             
    Elapsed Cycles                                                                      563456 Cycles         
    SPP Active Cycles                                                                     1913 Cycles         
    Scheduler(NS) Issue Cycles                                                             180 Cycles         
    ALU Active Cycles                                                                      208 Cycles         
    LSU Active Cycles                                                                      189 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000323 Inst/Cycle     
    Executed Instructions                                                                  180 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            66 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             90 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   82 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               8 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      90 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         20 Registers/Warp 
    Vector Register                                                                          8 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        43.38 MB/s           
    Global Memory Store Throughput                                                       49.57 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.98 Warps          
    Achieved Occupancy                                                                    6.23 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.23%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3>), Context 1, Stream 1, correlationId 1844
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                              9800 Ns             
    Elapsed Cycles                                                                      561152 Cycles         
    SPP Active Cycles                                                                     2084 Cycles         
    Scheduler(NS) Issue Cycles                                                             206 Cycles         
    ALU Active Cycles                                                                      226 Cycles         
    LSU Active Cycles                                                                      189 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000369 Inst/Cycle     
    Executed Instructions                                                                  206 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            66 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                          24 Instructions   
     -Multi-Lane Global Memory Instructions                                                 24 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            108 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  100 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               8 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     108 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         18 Registers/Warp 
    Vector Register                                                                         14 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        87.19 MB/s           
    Global Memory Store Throughput                                                       49.82 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           4.0 Warps          
    Achieved Occupancy                                                                    3.13 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (3.13%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3>), Context 1, Stream 1, correlationId 1853
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             10550 Ns             
    Elapsed Cycles                                                                      565760 Cycles         
    SPP Active Cycles                                                                     2226 Cycles         
    Scheduler(NS) Issue Cycles                                                             204 Cycles         
    ALU Active Cycles                                                                      250 Cycles         
    LSU Active Cycles                                                                      207 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000334 Inst/Cycle     
    Executed Instructions                                                                  204 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            78 Instructions   
    Single-Lane Control Flow Instructions                                                    4 Instructions   
    Multi-Lane Memory Instructions                                                          20 Instructions   
     -Multi-Lane Global Memory Instructions                                                 20 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             96 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   96 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      96 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         22 Registers/Warp 
    Vector Register                                                                         14 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.04 %              
    LSU--Global Memory Utilization                                                        0.04 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        23.14 MB/s           
    Global Memory Store Throughput                                                       46.28 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.97 Warps          
    Achieved Occupancy                                                                    6.23 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.23%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 1862
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                              9726 Ns             
    Elapsed Cycles                                                                      561152 Cycles         
    SPP Active Cycles                                                                     1710 Cycles         
    Scheduler(NS) Issue Cycles                                                             180 Cycles         
    ALU Active Cycles                                                                      210 Cycles         
    LSU Active Cycles                                                                      193 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000322 Inst/Cycle     
    Executed Instructions                                                                  180 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            66 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             90 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   82 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               8 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      90 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         20 Registers/Warp 
    Vector Register                                                                          8 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        43.93 MB/s           
    Global Memory Store Throughput                                                        50.2 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.97 Warps          
    Achieved Occupancy                                                                    6.22 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.22%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3>), Context 1, Stream 1, correlationId 1871
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             10553 Ns             
    Elapsed Cycles                                                                      571520 Cycles         
    SPP Active Cycles                                                                     4401 Cycles         
    Scheduler(NS) Issue Cycles                                                             204 Cycles         
    ALU Active Cycles                                                                      254 Cycles         
    LSU Active Cycles                                                                      189 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000361 Inst/Cycle     
    Executed Instructions                                                                  204 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            78 Instructions   
    Single-Lane Control Flow Instructions                                                    4 Instructions   
    Multi-Lane Memory Instructions                                                          20 Instructions   
     -Multi-Lane Global Memory Instructions                                                 20 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             96 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   96 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      96 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         22 Registers/Warp 
    Vector Register                                                                         14 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        23.13 MB/s           
    Global Memory Store Throughput                                                       46.27 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.96 Warps          
    Achieved Occupancy                                                                    6.22 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.22%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::cuda::detail::cub::DeviceReduceSingleTileKernel<at::cuda::detail::cub::DeviceReducePolicy<bool, int, int, at::cuda::detail::cub::Sum>::Policy600, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int>(at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int), Context 1, Stream 1, correlationId 1888
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             11346 Ns             
    Elapsed Cycles                                                                      663680 Cycles         
    SPP Active Cycles                                                                     6326 Cycles         
    Scheduler(NS) Issue Cycles                                                             521 Cycles         
    ALU Active Cycles                                                                      627 Cycles         
    LSU Active Cycles                                                                      430 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000787 Inst/Cycle     
    Executed Instructions                                                                  521 Instructions   
    Single-Lane Memory Instructions                                                         16 Instructions   
    Single-Lane ALU Instructions                                                           151 Instructions   
    Single-Lane Control Flow Instructions                                                   39 Instructions   
    Multi-Lane Memory Instructions                                                          37 Instructions   
     -Multi-Lane Global Memory Instructions                                                  8 Instructions   
     -Multi-Lane Shared Memory Instructions                                                 29 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            278 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  278 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     278 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [256, 1, 1]                
    Launched Warps                                                                           4 Warps          
    Threads                                                                                256 Threads        
    Scalar Register                                                                         18 Registers/Warp 
    Vector Register                                                                         48 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                    24 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.06 %              
    LSU--Global Memory Utilization                                                        0.02 %              
    LSU--Shared Memory Utilization                                                        0.04 %              
    Global Memory Load Throughput                                                        10.76 MB/s           
    Global Memory Store Throughput                                                        5.38 MB/s           
    Shared Memory Load Throughput                                                         5.38 MB/s           
    Shared Memory Store Throughput                                                       21.52 MB/s           
    Shared Store Bank Conflict                                                              72 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  0.2 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.87 Warps          
    Achieved Occupancy                                                                    6.15 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  21 Blocks         
    Block Limit SRF Per CU                                                                 113 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                32 Blocks         
    Theoretical Active Warps per CU                                                         84 Warps          
    Theoretical Occupancy                                                                65.62 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (65.62%) is limited by the number of
required VRF.The difference between calculated theoretical (65.62%) and measured achieved occupancy (6.15%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::cuda::detail::cub::DeviceCompactInitKernel<at::cuda::detail::cub::ScanTileState<int, true>, int*>(at::cuda::detail::cub::ScanTileState<int, true>, int, int*), Context 1, Stream 1, correlationId 1906
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             10100 Ns             
    Elapsed Cycles                                                                      561152 Cycles         
    SPP Active Cycles                                                                     1890 Cycles         
    Scheduler(NS) Issue Cycles                                                              90 Cycles         
    ALU Active Cycles                                                                      102 Cycles         
    LSU Active Cycles                                                                       66 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000161 Inst/Cycle     
    Executed Instructions                                                                   90 Instructions   
    Single-Lane Memory Instructions                                                          6 Instructions   
    Single-Lane ALU Instructions                                                            36 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                           6 Instructions   
     -Multi-Lane Global Memory Instructions                                                  6 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             40 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   40 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      40 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         16 Registers/Warp 
    Vector Register                                                                          4 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                          0.0 B/s            
    Global Memory Store Throughput                                                       36.26 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.96 Warps          
    Achieved Occupancy                                                                    6.22 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.22%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::cuda::detail::cub::DeviceSelectSweepKernel<at::cuda::detail::cub::DispatchSelectIf<at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>::PtxSelectIfPolicyT, at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>(at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, int), Context 1, Stream 1, correlationId 1913
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12673 Ns             
    Elapsed Cycles                                                                      717824 Cycles         
    SPP Active Cycles                                                                     4122 Cycles         
    Scheduler(NS) Issue Cycles                                                             454 Cycles         
    ALU Active Cycles                                                                      647 Cycles         
    LSU Active Cycles                                                                      520 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000648 Inst/Cycle     
    Executed Instructions                                                                  454 Instructions   
    Single-Lane Memory Instructions                                                         14 Instructions   
    Single-Lane ALU Instructions                                                           124 Instructions   
    Single-Lane Control Flow Instructions                                                   20 Instructions   
    Multi-Lane Memory Instructions                                                          54 Instructions   
     -Multi-Lane Global Memory Instructions                                                 14 Instructions   
     -Multi-Lane Shared Memory Instructions                                                 40 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            242 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  242 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     242 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         54 Registers/Warp 
    Vector Register                                                                         37 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                  3072 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.07 %              
    LSU--Global Memory Utilization                                                        0.02 %              
    LSU--Shared Memory Utilization                                                        0.05 %              
    Global Memory Load Throughput                                                         9.63 MB/s           
    Global Memory Store Throughput                                                      187.83 MB/s           
    Shared Memory Load Throughput                                                       712.79 MB/s           
    Shared Memory Store Throughput                                                      587.57 MB/s           
    Shared Store Bank Conflict                                                             102 Conflicts      
    Shared Load Bank Conflict                                                               78 Conflicts      
    L1VK Unit Utilization                                                                 0.19 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.99 Warps          
    Achieved Occupancy                                                                    6.24 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  55 Blocks         
    Block Limit SRF Per CU                                                                  75 Blocks         
    Block Limit Shared Mem Per CU                                                           42 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         84 Warps          
    Theoretical Occupancy                                                                65.62 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (65.62%) is limited by the required amount
of shared memory.The difference between calculated theoretical (65.62%) and measured achieved occupancy
(6.24%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::index_elementwise_kernel<128, 4, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}), Context 1, Stream 1, correlationId 1926
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             16250 Ns             
    Elapsed Cycles                                                                      934400 Cycles         
    SPP Active Cycles                                                                    10115 Cycles         
    Scheduler(NS) Issue Cycles                                                             158 Cycles         
    ALU Active Cycles                                                                      176 Cycles         
    LSU Active Cycles                                                                      127 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000163 Inst/Cycle     
    Executed Instructions                                                                  158 Instructions   
    Single-Lane Memory Instructions                                                         12 Instructions   
    Single-Lane ALU Instructions                                                            63 Instructions   
    Single-Lane Control Flow Instructions                                                   17 Instructions   
    Multi-Lane Memory Instructions                                                           3 Instructions   
     -Multi-Lane Global Memory Instructions                                                  3 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             63 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   63 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               0 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                8 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      55 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         50 Registers/Warp 
    Vector Register                                                                         13 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        78.88 MB/s           
    Global Memory Store Throughput                                                       15.02 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.14 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          5.22 Warps          
    Achieved Occupancy                                                                    4.08 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  81 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (4.08%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 1941
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                              9903 Ns             
    Elapsed Cycles                                                                      570368 Cycles         
    SPP Active Cycles                                                                     3287 Cycles         
    Scheduler(NS) Issue Cycles                                                             170 Cycles         
    ALU Active Cycles                                                                      198 Cycles         
    LSU Active Cycles                                                                      152 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000305 Inst/Cycle     
    Executed Instructions                                                                  170 Instructions   
    Single-Lane Memory Instructions                                                          4 Instructions   
    Single-Lane ALU Instructions                                                            66 Instructions   
    Single-Lane Control Flow Instructions                                                    2 Instructions   
    Multi-Lane Memory Instructions                                                          16 Instructions   
     -Multi-Lane Global Memory Instructions                                                 16 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                             82 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   74 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               8 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                      82 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         18 Registers/Warp 
    Vector Register                                                                          8 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.03 %              
    LSU--Global Memory Utilization                                                        0.03 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        24.65 MB/s           
    Global Memory Store Throughput                                                       24.65 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.24 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.97 Warps          
    Achieved Occupancy                                                                    6.23 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (6.23%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4>), Context 1, Stream 1, correlationId 1956
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             16950 Ns             
    Elapsed Cycles                                                                      977024 Cycles         
    SPP Active Cycles                                                                     8568 Cycles         
    Scheduler(NS) Issue Cycles                                                             319 Cycles         
    ALU Active Cycles                                                                      340 Cycles         
    LSU Active Cycles                                                                      118 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000325 Inst/Cycle     
    Executed Instructions                                                                  319 Instructions   
    Single-Lane Memory Instructions                                                         22 Instructions   
    Single-Lane ALU Instructions                                                           130 Instructions   
    Single-Lane Control Flow Instructions                                                   31 Instructions   
    Multi-Lane Memory Instructions                                                           8 Instructions   
     -Multi-Lane Global Memory Instructions                                                  3 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  5 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            128 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   98 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                              30 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                4 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     124 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                      [32, 1, 1]                
    Launched Warps                                                                           1 Warps          
    Threads                                                                                 32 Threads        
    Scalar Register                                                                        102 Registers/Warp 
    Vector Register                                                                         22 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     4 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Block Size --- Threads are executed in groups of 64 threads called warps. This kernel launch is
configured to execute 32 threads per block. Consequently, some threads in a warp are masked off and those
hardware resources are unused. Try changing the number of threads per block to be a multiple of 64 threads.
Use smaller thread blocks rather than one large thread block per multiprocessor if latency affects
performance. This is particularly beneficial to kernels that frequently call __syncthreads(). 
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         14.4 MB/s           
    Global Memory Store Throughput                                                         3.6 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                              15 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.14 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           4.0 Warps          
    Achieved Occupancy                                                                    3.12 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  80 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                               128 Blocks         
    Theoretical Active Warps per CU                                                         80 Warps          
    Theoretical Occupancy                                                                 62.5 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (62.5%) is limited by the number of required
SRF.The difference between calculated theoretical (62.5%) and measured achieved occupancy (3.12%) can be
the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4>), Context 1, Stream 1, correlationId 1971
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             17096 Ns             
    Elapsed Cycles                                                                     1036928 Cycles         
    SPP Active Cycles                                                                     8735 Cycles         
    Scheduler(NS) Issue Cycles                                                             319 Cycles         
    ALU Active Cycles                                                                      328 Cycles         
    LSU Active Cycles                                                                      118 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000307 Inst/Cycle     
    Executed Instructions                                                                  319 Instructions   
    Single-Lane Memory Instructions                                                         22 Instructions   
    Single-Lane ALU Instructions                                                           130 Instructions   
    Single-Lane Control Flow Instructions                                                   31 Instructions   
    Multi-Lane Memory Instructions                                                           8 Instructions   
     -Multi-Lane Global Memory Instructions                                                  3 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  5 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            128 Instructions   
     -Multi-Lane ALU Integer Instructions                                                   98 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                              30 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                4 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     124 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                      [32, 1, 1]                
    Launched Warps                                                                           1 Warps          
    Threads                                                                                 32 Threads        
    Scalar Register                                                                        102 Registers/Warp 
    Vector Register                                                                         22 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     4 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Block Size --- Threads are executed in groups of 64 threads called warps. This kernel launch is
configured to execute 32 threads per block. Consequently, some threads in a warp are masked off and those
hardware resources are unused. Try changing the number of threads per block to be a multiple of 64 threads.
Use smaller thread blocks rather than one large thread block per multiprocessor if latency affects
performance. This is particularly beneficial to kernels that frequently call __syncthreads(). 
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                        14.28 MB/s           
    Global Memory Store Throughput                                                        3.57 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                              15 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.13 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           4.0 Warps          
    Achieved Occupancy                                                                    3.13 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  80 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                               128 Blocks         
    Theoretical Active Warps per CU                                                         80 Warps          
    Theoretical Occupancy                                                                 62.5 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (62.5%) is limited by the number of required
SRF.The difference between calculated theoretical (62.5%) and measured achieved occupancy (3.13%) can be
the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 1980
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12623 Ns             
    Elapsed Cycles                                                                      721280 Cycles         
    SPP Active Cycles                                                                     4470 Cycles         
    Scheduler(NS) Issue Cycles                                                             268 Cycles         
    ALU Active Cycles                                                                      186 Cycles         
    LSU Active Cycles                                                                       71 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000372 Inst/Cycle     
    Executed Instructions                                                                  268 Instructions   
    Single-Lane Memory Instructions                                                         22 Instructions   
    Single-Lane ALU Instructions                                                           113 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           2 Instructions   
     -Multi-Lane Global Memory Instructions                                                  2 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            120 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  119 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               1 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       1 Instructions   
     +BFU Instructions                                                                     119 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         92 Registers/Warp 
    Vector Register                                                                          9 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         4.84 MB/s           
    Global Memory Store Throughput                                                        4.84 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.19 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           8.0 Warps          
    Achieved Occupancy                                                                    6.25 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  44 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         88 Warps          
    Theoretical Occupancy                                                                68.75 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (68.75%) is limited by the number of
required SRF.The difference between calculated theoretical (68.75%) and measured achieved occupancy (6.25%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 1989
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             13450 Ns             
    Elapsed Cycles                                                                      771968 Cycles         
    SPP Active Cycles                                                                    10580 Cycles         
    Scheduler(NS) Issue Cycles                                                             319 Cycles         
    ALU Active Cycles                                                                      233 Cycles         
    LSU Active Cycles                                                                       70 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000398 Inst/Cycle     
    Executed Instructions                                                                  319 Instructions   
    Single-Lane Memory Instructions                                                         26 Instructions   
    Single-Lane ALU Instructions                                                           121 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           3 Instructions   
     -Multi-Lane Global Memory Instructions                                                  3 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            158 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  157 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               1 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     158 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         94 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         9.08 MB/s           
    Global Memory Store Throughput                                                        4.54 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.17 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           8.0 Warps          
    Achieved Occupancy                                                                    6.25 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  43 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         86 Warps          
    Theoretical Occupancy                                                                67.19 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (67.19%) is limited by the number of
required SRF.The difference between calculated theoretical (67.19%) and measured achieved occupancy (6.25%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 2003
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             13853 Ns             
    Elapsed Cycles                                                                      778880 Cycles         
    SPP Active Cycles                                                                     5590 Cycles         
    Scheduler(NS) Issue Cycles                                                             324 Cycles         
    ALU Active Cycles                                                                      241 Cycles         
    LSU Active Cycles                                                                       79 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000395 Inst/Cycle     
    Executed Instructions                                                                  324 Instructions   
    Single-Lane Memory Instructions                                                         26 Instructions   
    Single-Lane ALU Instructions                                                           121 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           3 Instructions   
     -Multi-Lane Global Memory Instructions                                                  3 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            163 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  157 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               6 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       1 Instructions   
     +BFU Instructions                                                                     162 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         94 Registers/Warp 
    Vector Register                                                                         11 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         8.81 MB/s           
    Global Memory Store Throughput                                                        4.41 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.17 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.99 Warps          
    Achieved Occupancy                                                                    6.25 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  43 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         86 Warps          
    Theoretical Occupancy                                                                67.19 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (67.19%) is limited by the number of
required SRF.The difference between calculated theoretical (67.19%) and measured achieved occupancy (6.25%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 2012
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12150 Ns             
    Elapsed Cycles                                                                      715520 Cycles         
    SPP Active Cycles                                                                     4014 Cycles         
    Scheduler(NS) Issue Cycles                                                             272 Cycles         
    ALU Active Cycles                                                                      188 Cycles         
    LSU Active Cycles                                                                       73 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000383 Inst/Cycle     
    Executed Instructions                                                                  272 Instructions   
    Single-Lane Memory Instructions                                                         24 Instructions   
    Single-Lane ALU Instructions                                                           113 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           2 Instructions   
     -Multi-Lane Global Memory Instructions                                                  2 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            122 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  121 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               1 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     122 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         94 Registers/Warp 
    Vector Register                                                                          9 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         5.02 MB/s           
    Global Memory Store Throughput                                                        5.02 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.19 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          7.99 Warps          
    Achieved Occupancy                                                                    6.24 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  43 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         86 Warps          
    Theoretical Occupancy                                                                67.19 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (67.19%) is limited by the number of
required SRF.The difference between calculated theoretical (67.19%) and measured achieved occupancy (6.24%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 2026
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             11973 Ns             
    Elapsed Cycles                                                                      684416 Cycles         
    SPP Active Cycles                                                                     4007 Cycles         
    Scheduler(NS) Issue Cycles                                                             272 Cycles         
    ALU Active Cycles                                                                      189 Cycles         
    LSU Active Cycles                                                                       70 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000398 Inst/Cycle     
    Executed Instructions                                                                  272 Instructions   
    Single-Lane Memory Instructions                                                         24 Instructions   
    Single-Lane ALU Instructions                                                           113 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           2 Instructions   
     -Multi-Lane Global Memory Instructions                                                  2 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            122 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  121 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               1 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     122 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         94 Registers/Warp 
    Vector Register                                                                          9 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                          5.1 MB/s           
    Global Memory Store Throughput                                                         5.1 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                  0.2 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           4.0 Warps          
    Achieved Occupancy                                                                    3.13 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  43 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         86 Warps          
    Theoretical Occupancy                                                                67.19 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (67.19%) is limited by the number of
required SRF.The difference between calculated theoretical (67.19%) and measured achieved occupancy (3.13%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6}), Context 1, Stream 1, correlationId 2040
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                             12150 Ns             
    Elapsed Cycles                                                                      713216 Cycles         
    SPP Active Cycles                                                                     3978 Cycles         
    Scheduler(NS) Issue Cycles                                                             272 Cycles         
    ALU Active Cycles                                                                      189 Cycles         
    LSU Active Cycles                                                                       72 Cycles         
    L1 Vector Cache Active Cycles                                                         1344 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.0 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.000382 Inst/Cycle     
    Executed Instructions                                                                  272 Instructions   
    Single-Lane Memory Instructions                                                         24 Instructions   
    Single-Lane ALU Instructions                                                           113 Instructions   
    Single-Lane Control Flow Instructions                                                   11 Instructions   
    Multi-Lane Memory Instructions                                                           2 Instructions   
     -Multi-Lane Global Memory Instructions                                                  2 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                            122 Instructions   
     -Multi-Lane ALU Integer Instructions                                                  121 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                               1 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                     122 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                        [1, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                           2 Warps          
    Threads                                                                                128 Threads        
    Scalar Register                                                                         94 Registers/Warp 
    Vector Register                                                                          9 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.
    WRN: Small Grid --- The grid for this launch is configured to execute only 1 blocks, which is less than the
GPU's 16 CUs. This can underutilize some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to have at least one block per
multiprocessor or increase the size of the grid to fully utilize the available hardware resources.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                       0.01 %              
    LSU--Global Memory Utilization                                                        0.01 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                         5.02 MB/s           
    Global Memory Store Throughput                                                        5.02 MB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                 0.19 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                    100.0 %              
    L2 Cache Access to Memory                                                                0 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                           8.0 Warps          
    Achieved Occupancy                                                                    6.25 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                  43 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                         86 Warps          
    Theoretical Occupancy                                                                67.19 %              
    Waves Per CU                                                                           0.0                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (67.19%) is limited by the number of
required SRF.The difference between calculated theoretical (67.19%) and measured achieved occupancy (6.25%)
can be the result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> >, at::detail::Array<char*, 3>), Context 1, Stream 1, correlationId 2404
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            287803 Ns             
    Elapsed Cycles                                                                    17143040 Cycles         
    SPP Active Cycles                                                                 16678629 Cycles         
    Scheduler(NS) Issue Cycles                                                         8632114 Cycles         
    ALU Active Cycles                                                                 10118241 Cycles         
    LSU Active Cycles                                                                 16471446 Cycles         
    L1 Vector Cache Active Cycles                                                      4141282 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.645495 Inst/Cycle     
    Executed Instructions                                                             11075328 Instructions   
    Single-Lane Memory Instructions                                                     240768 Instructions   
    Single-Lane ALU Instructions                                                       3531264 Instructions   
    Single-Lane Control Flow Instructions                                               401280 Instructions   
    Multi-Lane Memory Instructions                                                      963072 Instructions   
     -Multi-Lane Global Memory Instructions                                             963072 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                        5938944 Instructions   
     -Multi-Lane ALU Integer Instructions                                              4654848 Instructions   
     -Multi-Lane ALU Half Float Instructions                                            321024 Instructions   
     -Multi-Lane ALU Single Float Instructions                                          963072 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                 5938944 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                    [40128, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                       80256 Warps          
    Threads                                                                            5136384 Threads        
    Scalar Register                                                                         20 Registers/Warp 
    Vector Register                                                                         14 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      96.08 %              
    LSU--Global Memory Utilization                                                       96.08 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       265.94 GB/s           
    Global Memory Store Throughput                                                      265.94 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                24.16 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                       49.83 %              
    L2 Cache Hit Rate                                                                     0.12 %              
    L2 Cache Access to Memory                                                          1926080 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                        112.64 Warps          
    Achieved Occupancy                                                                    88.0 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                         39.19                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::modern::elementwise_kernel<at::native::AbsFunctor<c10::Half>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<c10::Half>, at::detail::Array<char*, 2>), Context 1, Stream 1, correlationId 2419
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            210700 Ns             
    Elapsed Cycles                                                                    13409408 Cycles         
    SPP Active Cycles                                                                 12890264 Cycles         
    Scheduler(NS) Issue Cycles                                                         7987213 Cycles         
    ALU Active Cycles                                                                  8392116 Cycles         
    LSU Active Cycles                                                                 12449318 Cycles         
    L1 Vector Cache Active Cycles                                                      3192816 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    INF: High Throughput --- The kernel is utilizing greater than 80% of the available compute or memory
performance of the device. To further improve performance, work will likely need to be shifted from the
most utilized to another unit. Start by analyzing workloads in the Memory Access section.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.696607 Inst/Cycle     
    Executed Instructions                                                              9309696 Instructions   
    Single-Lane Memory Instructions                                                     160512 Instructions   
    Single-Lane ALU Instructions                                                       3531264 Instructions   
    Single-Lane Control Flow Instructions                                               401280 Instructions   
    Multi-Lane Memory Instructions                                                      642048 Instructions   
     -Multi-Lane Global Memory Instructions                                             642048 Instructions   
     -Multi-Lane Shared Memory Instructions                                                  0 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                        4574592 Instructions   
     -Multi-Lane ALU Integer Instructions                                              3611520 Instructions   
     -Multi-Lane ALU Half Float Instructions                                            321024 Instructions   
     -Multi-Lane ALU Single Float Instructions                                          642048 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                                0 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                 4574592 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                    [40128, 1, 1]                
    Block Size                                                                     [128, 1, 1]                
    Launched Warps                                                                       80256 Warps          
    Threads                                                                            5136384 Threads        
    Scalar Register                                                                         18 Registers/Warp 
    Vector Register                                                                         10 Registers/Thread
    Dynamic Shared Memory                                                                    0 Bytes/Block    
    Static Shared Memory                                                                     0 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      92.84 %              
    LSU--Global Memory Utilization                                                       92.84 %              
    LSU--Shared Memory Utilization                                                         0.0 %              
    Global Memory Load Throughput                                                       181.63 GB/s           
    Global Memory Store Throughput                                                      363.26 GB/s           
    Shared Memory Load Throughput                                                          0.0 B/s            
    Shared Memory Store Throughput                                                         0.0 B/s            
    Shared Store Bank Conflict                                                               0 Conflicts      
    Shared Load Bank Conflict                                                                0 Conflicts      
    L1VK Unit Utilization                                                                23.81 %              
    L1 Vector Cache Load Hit Rate                                                          0.0 %              
    L1 Vector Cache Store Hit Rate                                                        50.0 %              
    L2 Cache Hit Rate                                                                      0.0 %              
    L2 Cache Access to Memory                                                          1284112 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                         30.97 Warps          
    Achieved Occupancy                                                                    24.2 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                 128 Blocks         
    Block Limit SRF Per CU                                                                 128 Blocks         
    Block Limit Shared Mem Per CU                                                          128 Blocks         
    Block Limit Warps Per CU                                                                64 Blocks         
    Theoretical Active Warps per CU                                                        128 Warps          
    Theoretical Occupancy                                                                100.0 %              
    Waves Per CU                                                                         39.19                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    INF: Occupancy Limiters --- This kernel's theoretical occupancy is not impacted by any block limit. The
difference between calculated theoretical (100.0%) and measured achieved occupancy (24.2%) can be the
result of warp scheduling overheads or workload imbalances during the kernel execution.

  Kernel: void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::MaxNanFunctor<c10::Half> >, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::MaxNanFunctor<c10::Half> >, unsigned int, c10::Half, 4>), Context 1, Stream 1, correlationId 2437
    Section: GPU Speed Of Light
    ------------------------------------------------------------------------- ---------------- -------------
    Duration                                                                            189226 Ns             
    Elapsed Cycles                                                                    11159552 Cycles         
    SPP Active Cycles                                                                  7945235 Cycles         
    Scheduler(NS) Issue Cycles                                                         5271809 Cycles         
    ALU Active Cycles                                                                  6035518 Cycles         
    LSU Active Cycles                                                                  6044287 Cycles         
    L1 Vector Cache Active Cycles                                                      1727400 Cycles         
    ------------------------------------------------------------------------- ---------------- -------------
    INF: High-level overview of the usage for compute and memory resources of the GPU.
    WRN: Small Grid --- This kernel grid is too small to fill the available resources on this device, resulting
in only 0.99 full waves across all CUs. Look at [Launch Statistics] for more details.

    Section: Instruction Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Executed IPC                                                                      0.573047 Inst/Cycle     
    Executed Instructions                                                              6410092 Instructions   
    Single-Lane Memory Instructions                                                      31758 Instructions   
    Single-Lane ALU Instructions                                                       1696650 Instructions   
    Single-Lane Control Flow Instructions                                               644461 Instructions   
    Multi-Lane Memory Instructions                                                       98110 Instructions   
     -Multi-Lane Global Memory Instructions                                              84285 Instructions   
     -Multi-Lane Shared Memory Instructions                                              13825 Instructions   
     -Multi-Lane SME Memory Instructions                                                     0 Instructions   
    Multi-Lane ALU Instructions                                                        3939113 Instructions   
     -Multi-Lane ALU Integer Instructions                                              1937603 Instructions   
     -Multi-Lane ALU Half Float Instructions                                                 0 Instructions   
     -Multi-Lane ALU Single Float Instructions                                         2001510 Instructions   
     +Multi-Lane Matrix Instructions                                                         0 Instructions   
     +Multi-Lane 4-Pass Multiply Instructions                                            12640 Instructions   
     +SFU Instructions                                                                       0 Instructions   
     +BFU Instructions                                                                 3926473 Instructions   
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Statistics of Instructions issued and executed by the kernel.

    Section: Launch Statistics
    ------------------------------------------------------------------------- ---------------- -------------
    Grid Size                                                                       [1, 79, 1]                
    Block Size                                                                    [1024, 1, 1]                
    Launched Warps                                                                        1264 Warps          
    Threads                                                                              80896 Threads        
    Scalar Register                                                                        102 Registers/Warp 
    Vector Register                                                                         22 Registers/Thread
    Dynamic Shared Memory                                                                 2048 Bytes/Block    
    Static Shared Memory                                                                     4 Bytes/Block    
    ------------------------------------------------------------------------- ---------------- -------------
    INF: General information of Launch Statistics.

    Section: Memory Access
    ------------------------------------------------------------------------- ---------------- -------------
    LSU Utilization                                                                      54.16 %              
    LSU--Global Memory Utilization                                                       52.96 %              
    LSU--Shared Memory Utilization                                                        1.39 %              
    Global Memory Load Throughput                                                       405.28 GB/s           
    Global Memory Store Throughput                                                       25.48 MB/s           
    Shared Memory Load Throughput                                                         3.83 GB/s           
    Shared Memory Store Throughput                                                        7.04 GB/s           
    Shared Store Bank Conflict                                                           33891 Conflicts      
    Shared Load Bank Conflict                                                             7347 Conflicts      
    L1VK Unit Utilization                                                                15.48 %              
    L1 Vector Cache Load Hit Rate                                                         50.1 %              
    L1 Vector Cache Store Hit Rate                                                         0.0 %              
    L2 Cache Hit Rate                                                                     0.02 %              
    L2 Cache Access to Memory                                                           642054 Requests       
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Summary of Memory access operations.

    Section: Occupancy
    ------------------------------------------------------------------------- ---------------- -------------
    Achieved Active Warps Per CU                                                          52.5 Warps          
    Achieved Occupancy                                                                   41.01 %              
    Block Limit CU                                                                         128 Blocks         
    Block Limit VRF Per CU                                                                  11 Blocks         
    Block Limit SRF Per CU                                                                   5 Blocks         
    Block Limit Shared Mem Per CU                                                           63 Blocks         
    Block Limit Warps Per CU                                                                 8 Blocks         
    Theoretical Active Warps per CU                                                         80 Warps          
    Theoretical Occupancy                                                                 62.5 %              
    Waves Per CU                                                                          0.99                
    ------------------------------------------------------------------------- ---------------- -------------
    INF: Occupancy is defined as the ratio of active warps on an NR to the maximum number(8) of active warps
supported by the NR.
    WRN: Occupancy Limiters --- This kernel's theoretical occupancy (62.5%) is limited by the number of required
SRF.The difference between calculated theoretical (62.5%) and measured achieved occupancy (41.01%) can be
the result of warp scheduling overheads or workload imbalances during the kernel execution.

