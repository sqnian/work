nohup: ignoring input
test op 

output_pt:tensor([0.1268, 0.0000, 0.2979, 0.2075, 0.0000, 0.0000, 0.1240, 0.0099, 0.0000,
        0.0629, 0.0126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1216, 0.0000,
        0.0000, 0.4124, 0.3174, 0.2001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.1678, 0.0000, 0.1964, 0.1820, 0.0000, 0.0000, 0.2285, 0.0276, 0.0966,
        0.0000, 0.0000, 0.6958, 0.1216, 0.0000, 0.0997, 0.0000, 0.2795, 0.1609,
        0.0000, 0.0891, 0.2732, 0.0000, 0.0000, 0.0000, 0.0000, 0.1895, 0.3425,
        0.2578, 0.0000, 0.0000, 0.0000, 0.4500, 0.0000, 0.1271, 0.0106, 0.0602,
        0.0000, 0.2917, 0.0000, 0.1383, 0.0000, 0.0000, 0.1998, 0.0000, 0.0000,
        0.6367, 0.0000, 0.0000, 0.4355, 0.0000, 0.1749, 0.0000, 0.0000, 0.1642,
        0.4077, 0.0078, 0.0000, 0.4426, 0.0000, 0.1885, 0.1078, 0.1750, 0.0000,
        0.0000, 0.0273, 0.0922, 0.0000, 0.0000, 0.1260, 0.6016, 0.0000, 0.0000,
        0.0378], device='cuda:0', dtype=torch.float16,
       grad_fn=<SliceBackward0>)

output_pt shape:torch.Size([24, 176, 19, 256])
output_cu:tensor([0.1270, 0.0000, 0.2979, 0.2075, 0.0000, 0.0000, 0.1241, 0.0099, 0.0000,
        0.0628, 0.0126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1216, 0.0000,
        0.0000, 0.4124, 0.3174, 0.2002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.1681, 0.0000, 0.1964, 0.1820, 0.0000, 0.0000, 0.2286, 0.0277, 0.0966,
        0.0000, 0.0000, 0.6958, 0.1216, 0.0000, 0.0996, 0.0000, 0.2793, 0.1610,
        0.0000, 0.0890, 0.2732, 0.0000, 0.0000, 0.0000, 0.0000, 0.1895, 0.3428,
        0.2576, 0.0000, 0.0000, 0.0000, 0.4500, 0.0000, 0.1270, 0.0106, 0.0602,
        0.0000, 0.2917, 0.0000, 0.1384, 0.0000, 0.0000, 0.1997, 0.0000, 0.0000,
        0.6367, 0.0000, 0.0000, 0.4355, 0.0000, 0.1750, 0.0000, 0.0000, 0.1642,
        0.4077, 0.0078, 0.0000, 0.4429, 0.0000, 0.1886, 0.1078, 0.1750, 0.0000,
        0.0000, 0.0272, 0.0921, 0.0000, 0.0000, 0.1260, 0.6016, 0.0000, 0.0000,
        0.0378], device='cuda:0', dtype=torch.float16)

output_cu :torch.Size([24, 176, 19, 256])

diff max:0.0009765625

==100439== IXPROF is profiling process 100439, command: /usr/local/bin/python3 two_test_pad.py 
==100439== Profiling application: /usr/local/bin/python3 two_test_pad.py 
==100439== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   24.05%  7.7608ms       111  69.916us  58.626us  213.62us  [CUDA memcpy DtoH]
                   20.70%  6.6813ms         2  3.3407ms  2.4201ms  4.2612ms  void cuinfer::impl::kernel::implConvolution2DTcuKernelHalfCommon<256, 256, 64, 64, 64, true, false, 1, 1, 0, 0, float, __half, __half, float>(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, __half*, float const*, float, float, float, float, __half const*)
                   11.75%  3.7912ms         1  3.7912ms  3.7912ms  3.7912ms  void cudnn::impl::MR::kernel::implConvolution2DTcuKernelHalfTemplateDB<256, 256, 64, 64, 64, 256, true, false, float, __half, __half, float, float, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5> >(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half const*, float const*, int, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5>, int)
                    9.50%  3.0663ms         2  1.5331ms  609.90us  2.4564ms  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> > const&)::{lambda(int)#6})
                    9.00%  2.9039ms         1  2.9039ms  2.9039ms  2.9039ms  void cudnn::impl::MR::kernel::implConvolution2DKernel<3, 3, 32, float, __half, __half, float, float, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5> >(__half const*, __half const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half const*, float const*, int, cudnn::impl::kernel::ActivationFwdOp<(cudnnActivationMode_t)5>)
                    8.30%  2.6794ms         4  669.85us  9.4500us  2.6508ms  [CUDA memcpy HtoD]
                    4.42%  1.4284ms         2  714.21us  32.203us  1.3962ms  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#6})
                    3.26%  1.0543ms         2  527.17us  212.57us  841.77us  void at::native::modern::elementwise_kernel<at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::clamp_min_scalar_kernel_impl(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#16}::operator()() const::{lambda(c10::Half)#1}, at::detail::Array<char*, 2>)
                    2.35%  759.02us         2  379.51us  51.973us  707.05us  void cudnn::impl::kernel::IxdnnTransformTensorKernelHalfOpt<64u, 128u, 32u>(unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, short const*, short*)
                    1.04%  337.85us         1  337.85us  337.85us  337.85us  void ker_output_data<__half>(__half*, __half*)
                    0.89%  288.72us         1  288.72us  288.72us  288.72us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::AddFunctor<float> >, at::detail::Array<char*, 3>)
                    0.79%  255.04us         3  85.016us  11.076us  209.17us  void at::native::modern::elementwise_kernel<at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<c10::Half>, at::detail::Array<char*, 1>)
                    0.65%  209.79us         1  209.79us  209.79us  209.79us  void at::native::modern::elementwise_kernel<at::native::AbsFunctor<c10::Half>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<c10::Half>, at::detail::Array<char*, 2>)
                    0.58%  190.15us         1  190.15us  190.15us  190.15us  void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::MaxNanFunctor<c10::Half> >, unsigned int, c10::Half, 4> >(at::native::ReduceOp<c10::Half, at::native::func_wrapper_t<c10::Half, at::native::MaxNanFunctor<c10::Half> >, unsigned int, c10::Half, 4>)
                    0.41%  135.45us         1  135.45us  135.45us  135.45us  [CUDA memset]
                    0.19%  63.253us         1  63.253us  63.253us  63.253us  void ker_input_pad<__half>(__half*, __half*, int, int, int, int)
                    0.19%  62.769us         4  15.692us  10.923us  28.473us  void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#20}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})
                    0.17%  57.700us         1  57.700us  57.700us  57.700us  _ZN2at6native12_GLOBAL__N_143distribution_elementwise_grid_stride_kernelIfLi4EZNS0_9templates4cuda20normal_and_transformIN3c104HalfEfLm4EPNS_17CUDAGeneratorImplEZZZNS4_13normal_kernelIS9_EEvRNS_6TensorEddT_ENKUlvE_clEvENKUlvE4_clEvEUlfE_EEvRNS_18TensorIteratorBaseET2_T3_EUlP24curandStatePhilox4_32_10E0_ZNS1_27distribution_nullary_kernelIS7_fLi4ES9_SN_SG_EEvSI_SJ_RKSK_T4_EUlifE_EEviNS_15PhiloxCudaStateET1_SJ_
                    0.15%  49.675us         4  12.418us  11.823us  12.799us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareGTFunctor<float> > const&)::{lambda(int)#6})
                    0.14%  47.673us         4  11.918us  11.623us  12.200us  void at::native::legacy::elementwise_kernel<512, 1, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_kernel_impl<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#3}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})
                    0.12%  41.196us         4  10.299us  9.8230us  10.800us  void at::native::modern::elementwise_kernel<at::native::AbsFunctor<float>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<float>, at::detail::Array<char*, 2>)
                    0.12%  41.123us         4  10.280us  9.9000us  10.673us  void at::native::modern::elementwise_kernel<at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> >, at::detail::Array<char*, 2>)
                    0.11%  35.823us         2  17.911us  17.123us  18.700us  void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4>)
                    0.10%  34.303us         2  17.151us  16.800us  17.503us  void at::native::reduce_kernel<1024, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MinNanFunctor<float> >, unsigned int, float, 4>)
                    0.10%  33.506us         2  16.753us  16.353us  17.153us  void at::native::index_elementwise_kernel<128, 4, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, void at::native::gpu_index_kernel<void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, void at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})
                    0.08%  28.503us         2  14.251us  13.850us  14.653us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::DivFunctor<float> > const&)::{lambda(int)#6})
                    0.08%  28.346us         2  14.173us  13.850us  14.496us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, bool, at::native::CompareNEFunctor<float> > const&)::{lambda(int)#6})
                    0.07%  25.800us         2  12.900us  12.700us  13.100us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::ceil_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#6})
                    0.07%  25.649us         2  12.824us  12.799us  12.850us  void at::cuda::detail::cub::DeviceSelectSweepKernel<at::cuda::detail::cub::DispatchSelectIf<at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>::PtxSelectIfPolicyT, at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, false>(at::cuda::detail::cub::CountingInputIterator<long, long>, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, long*, int*, at::cuda::detail::cub::ScanTileState<int, true>, at::cuda::detail::cub::NullType, at::cuda::detail::cub::NullType, int, int)
                    0.07%  24.702us         2  12.351us  12.326us  12.376us  void at::native::legacy::elementwise_kernel<128, 4, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6}>(int, void at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, bool, at::native::CompareLTFunctor<float> > const&)::{lambda(int)#6})
                    0.07%  23.879us         2  11.939us  11.703us  12.176us  void at::cuda::detail::cub::DeviceReduceSingleTileKernel<at::cuda::detail::cub::DeviceReducePolicy<bool, int, int, at::cuda::detail::cub::Sum>::Policy600, at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int>(at::cuda::detail::cub::TransformInputIterator<bool, at::native::(anonymous namespace)::NonZeroOp<bool>, bool*, long>, int*, int, at::cuda::detail::cub::Sum, int)
                    0.06%  21.876us         2  10.938us  10.900us  10.976us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, at::detail::Array<char*, 3>)
                    0.06%  21.773us         2  10.886us  10.550us  11.223us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::MulFunctor<bool> >, at::detail::Array<char*, 3>)
                    0.06%  21.553us         2  10.776us  10.750us  10.803us  void at::native::modern::elementwise_kernel<at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, bool, at::native::CompareEqFunctor<float> >, at::detail::Array<char*, 3>)
                    0.06%  21.476us         2  10.738us  10.600us  10.876us  void at::cuda::detail::cub::DeviceCompactInitKernel<at::cuda::detail::cub::ScanTileState<int, true>, int*>(at::cuda::detail::cub::ScanTileState<int, true>, int, int*)
                    0.03%  12.823us         1  12.823us  12.823us  12.823us  void ker_weight_pad_trans<__half>(__half*, __half*, int)
      API Calls:   86.27%  5.61913s        64  87.799ms  174.00ns  1.48511s  cuModuleLoadData
                   12.91%  841.49ms         8  105.18ms  1.5640us  841.45ms  cudaFuncGetAttributes
                    0.28%  18.599ms       115  161.73us  136.91us  408.99us  cudaStreamSynchronize
                    0.26%  17.459ms         9  1.9400ms  951.00ns  2.5949ms  cudaSetDevice
                    0.13%  8.7027ms       115  75.675us  2.8910us  2.5369ms  cudaMemcpyAsync
                    0.10%  6.7387ms         2  3.3693ms  2.2010us  6.7365ms  cudaFree
                    0.00%  474.68us         1  474.68us  474.68us  474.68us  cudaMemsetAsync
                    0.00%  285.02us        68  4.1910us  1.4120us  24.998us  cudaLaunchKernel
                    0.00%  223.56us      1071  208.00ns  176.00ns  1.2170us  cudaGetDevice
                    0.00%  200.07us         7  28.582us  1.5570us  86.487us  cudaMalloc
                    0.00%  53.963us        18  2.9970us  713.00ns  9.6360us  cudaGetDeviceProperties
                    0.00%  16.189us        38  426.00ns  211.00ns  3.0170us  cudaStreamIsCapturing
                    0.00%  12.347us        68  181.00ns  54.000ns  506.00ns  cudaGetLastError
                    0.00%  5.9020us        33  178.00ns  44.000ns  4.2630us  cuDevicePrimaryCtxGetState
                    0.00%  5.8110us         3  1.9370us  83.000ns  5.4010us  cudaGetDeviceCount
                    0.00%  4.4460us         2  2.2230us  2.2070us  2.2390us  cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
                    0.00%  3.4710us         6  578.00ns  190.00ns  1.0250us  cudaDeviceGetAttribute
                    0.00%  1.4570us         6  242.00ns  63.000ns  726.00ns  cudaPeekAtLastError
